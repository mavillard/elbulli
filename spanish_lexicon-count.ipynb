{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "import networkx as nx\n",
    "import nltk\n",
    "from nltk.corpus import cess_esp\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.util import ngrams\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "# client.drop_database('lexicon')\n",
    "db = client.lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.3 s, sys: 88 ms, total: 12.4 s\n",
      "Wall time: 18.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('data/sp_lexicon.csv') as f:\n",
    "    reader = csv.reader(\n",
    "        f,\n",
    "        delimiter=' ',\n",
    "    )\n",
    "    docs = []\n",
    "    count = 0\n",
    "    for row in reader:\n",
    "        for i in range(1, len(row[1:]), 2):\n",
    "            entry = {}\n",
    "            entry['flexion'] = row[0].lower()\n",
    "            entry['lemma'] = row[i].lower()\n",
    "            entry['eagle'] = row[i+1].lower()\n",
    "            docs.append(entry)\n",
    "            count += 1\n",
    "        if count % 1000 == 0:\n",
    "            db.es_lexicon.insert_many(docs)\n",
    "            docs = []\n",
    "    db.es_lexicon.insert_many(docs)\n",
    "    docs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "668825"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.es_lexicon.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagged_sp_sents = cess_esp.tagged_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "size = int(len(tagged_sp_sents) * 0.1)\n",
    "train_sp_sents = tagged_sp_sents[size:]\n",
    "test_sp_sents = tagged_sp_sents[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tagged_sp_sents) == len(train_sp_sents) + len(test_sp_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_sp_words = cess_esp.tagged_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sps00', 25272),\n",
       " ('ncms000', 11428),\n",
       " ('Fc', 11420),\n",
       " ('ncfs000', 11008),\n",
       " ('da0fs0', 6838),\n",
       " ('da0ms0', 6012),\n",
       " ('rg', 5937),\n",
       " ('Fp', 5866),\n",
       " ('cc', 5854),\n",
       " ('ncmp000', 5711)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = [tag for (word, tag) in tagged_sp_words]\n",
    "most_freq_tags = nltk.FreqDist(tags)\n",
    "most_freq_tags.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_tag = 'ncms000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t0 = nltk.DefaultTagger(None)\n",
    "t1 = nltk.UnigramTagger(train_sp_sents, backoff=t0)\n",
    "t2 = nltk.BigramTagger(train_sp_sents, backoff=t1)\n",
    "sp_tagger = nltk.TrigramTagger(train_sp_sents, backoff=t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8809982486865149"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_tagger.evaluate(test_sp_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag_mapping = {\n",
    "    'a': 'adj',\n",
    "    'r': 'adv',\n",
    "    'd': 'det',\n",
    "    'n': 'noun',\n",
    "    'v': 'verb',\n",
    "    'p': 'pron',\n",
    "    'c': 'conj',\n",
    "    'i': 'interj',\n",
    "    's': 'prep',\n",
    "    'f': 'punt',\n",
    "    'z': 'num',\n",
    "    'w': 'date-time',\n",
    "}\n",
    "\n",
    "def has_category(category, entries):\n",
    "    return category in map(lambda x: tag_mapping[x['eagle'][0]], entries)\n",
    "\n",
    "def is_number(x):\n",
    "    return x in ['un', 'una', 'dos', 'tres', 'cuatro', 'cinco', 'seis', 'siete', 'ocho', 'nueve']\n",
    "\n",
    "def ingredient_tagger(x):\n",
    "    result = []\n",
    "    tokens = nltk.word_tokenize(x)\n",
    "    if len(tokens) == 1:\n",
    "        result.append((x, 'noun'))\n",
    "    else:\n",
    "        tags = sp_tagger.tag(tokens)\n",
    "        for token, tag in tags:\n",
    "            if not tag:\n",
    "                res = list(db.es_lexicon.find({'flexion': token}))\n",
    "                if res:\n",
    "                    if has_category('noun', res):\n",
    "                        tag = 'noun'\n",
    "                    elif has_category('adj', res):\n",
    "                        tag = 'adj'\n",
    "                    elif has_category('verb', res):\n",
    "                        tag = 'verb'\n",
    "                    elif has_category('num', res):\n",
    "                        tag = 'num'\n",
    "                    else:\n",
    "                        tag = tag_mapping[res[0]['eagle'][0]]\n",
    "                else:\n",
    "                    tag = 'noun'\n",
    "                result.append((token, tag))\n",
    "            elif is_number(token):\n",
    "                result.append((token, 'num'))\n",
    "            else:\n",
    "                result.append((token, tag_mapping[tag.lower()[0]]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def first(cat, entries):\n",
    "    for e in entries:\n",
    "        if e['eagle'][0] == cat:\n",
    "            break\n",
    "    return e\n",
    "\n",
    "def singularize(x):\n",
    "    sing = x\n",
    "    if x.endswith('s'):\n",
    "        if x == 'los':\n",
    "            sing = 'el'\n",
    "        elif x == 'dos':\n",
    "            sing = 'dos'\n",
    "        else:\n",
    "            res = list(db.es_lexicon.find({'flexion': x}))\n",
    "            if res:\n",
    "                if has_category('noun', res):\n",
    "                    r = first('n', res)\n",
    "                    eagle = r['eagle'][:3] + 's' + r['eagle'][4:]\n",
    "                elif has_category('adj', res):\n",
    "                    r = first('a', res)\n",
    "                    eagle = r['eagle'][:4] + 's' + r['eagle'][5:]\n",
    "                elif has_category('verb', res):\n",
    "                    r = first('v', res)\n",
    "                    eagle = r['eagle'][:5] + 's' + r['eagle'][6:]\n",
    "                elif has_category('det', res):\n",
    "                    r = first('d', res)\n",
    "                    eagle = r['eagle'][:4] + 's' + r['eagle'][5:]\n",
    "                elif has_category('pron', res):\n",
    "                    r = first('p', res)\n",
    "                    eagle = r['eagle'][:4] + 's' + r['eagle'][5:]\n",
    "                elif has_category('prep', res):\n",
    "                    r = first('s', res)\n",
    "                    eagle = r['eagle'][:3] + 's' + r['eagle'][4:]\n",
    "                else:\n",
    "                    r = res[0]\n",
    "                    eagle = r['eagle']\n",
    "                lemma = r['lemma']\n",
    "                s = db.es_lexicon.find_one({'lemma': lemma, 'eagle': eagle})\n",
    "                if s:\n",
    "                    sing = s['flexion']\n",
    "    return sing\n",
    "\n",
    "def lemmatize(ingredient):\n",
    "    tokens = nltk.word_tokenize(ingredient)\n",
    "    return ' '.join((map(singularize, tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Numbers\n",
    "def numbers(x):\n",
    "    return x.replace(' 1 ', ' uno ') \\\n",
    "            .replace(' 2 ', ' dos ') \\\n",
    "            .replace(' 3 ', ' tres ') \\\n",
    "            .replace(' 4 ', ' cuatro ') \\\n",
    "            .replace(' 5 ', ' cinco ') \\\n",
    "            .replace(' 6 ', ' seis ') \\\n",
    "            .replace(' 7 ', ' siete ') \\\n",
    "            .replace(' 8 ', ' ocho ') \\\n",
    "            .replace(' 9 ', ' nueve ')\n",
    "\n",
    "# Accent marks on vowels - {'á', 'ã', 'ç', 'è', 'é', 'ê', 'í', 'ñ', 'ò', 'ó', 'ú', 'ü', 'ō'}\n",
    "def accent_marks(x):\n",
    "    return x.replace('á', 'a') \\\n",
    "            .replace('ã', 'a') \\\n",
    "            .replace('è', 'e') \\\n",
    "            .replace('é', 'e') \\\n",
    "            .replace('ê', 'e') \\\n",
    "            .replace('í', 'i') \\\n",
    "            .replace('ò', 'o') \\\n",
    "            .replace('ó', 'o') \\\n",
    "            .replace('ō', 'o') \\\n",
    "            .replace('ú', 'u') \\\n",
    "            .replace('ü', 'u')\n",
    "\n",
    "# Non-ascii consonants - {'á', 'ã', 'ç', 'è', 'é', 'ê', 'í', 'ñ', 'ò', 'ó', 'ú', 'ü', 'ō'}\n",
    "def nonascii_consonants(x):\n",
    "    return x.replace('ç', 'c') \\\n",
    "            .replace('ñ', 'n')\n",
    "    \n",
    "# Dashes (-)\n",
    "def dashes1(x):\n",
    "    return x.replace('-', '')\n",
    "\n",
    "def dashes2(x):\n",
    "    return x.replace('-', ' ')\n",
    "\n",
    "# POS tags\n",
    "# ADJETIVOS .... A ADJ ...... X\n",
    "# ADVERBIOS .... R ADV\n",
    "# DETERMINANTES  D DET\n",
    "# NOMBRES ...... N NOUN ..... X\n",
    "# VERBOS ....... V VERB ..... X\n",
    "# PRONOMBRES ... P PRON\n",
    "# CONJUNCIONES . C CONJ\n",
    "# INTERJECCIONES I INTERJ\n",
    "# PREPOSICIONES  S PREP\n",
    "# PUNTUACIÓN ... F PUNTUATION\n",
    "# NUMERALES .... Z NUM ...... X\n",
    "# FECHAS Y HORAS W DATE-TIME\n",
    "def pos_tags(x):\n",
    "    tags = ingredient_tagger(x)\n",
    "    filtered = [token\n",
    "                for token, tag in tags\n",
    "                if tag in ['num', 'verb', 'adj', 'noun']\n",
    "               ]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "def singular(x):\n",
    "    return lemmatize(x)\n",
    "\n",
    "funcs = [singular, pos_tags, numbers, accent_marks, nonascii_consonants, dashes1, dashes2]\n",
    "combinations = []\n",
    "for i in range(1, len(funcs) + 1):\n",
    "    combinations.append(list(itertools.combinations(funcs, i)))\n",
    "combinations = [c for comb in combinations for c in comb]\n",
    "\n",
    "# def normalize(ingredient): # time consuming\n",
    "#     result = set()\n",
    "#     for c in combinations:\n",
    "#         x = ingredient\n",
    "#         for f in c:\n",
    "#             x = f(x)\n",
    "#         result.add(x)\n",
    "#     return result\n",
    "\n",
    "def normalize(ingredient): # dynamic programming version\n",
    "    d = {}\n",
    "    for f in funcs:\n",
    "        d[f] = {}\n",
    "    result = set()\n",
    "    for c in combinations:\n",
    "        x = ingredient\n",
    "        for f in c:\n",
    "            if not x in d[f]:\n",
    "                d[f][x] = f(x)\n",
    "            x = d[f][x]\n",
    "        result.add(x)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_spanish_ingredients_file(filename):\n",
    "    return filename.startswith('es_') and filename.endswith('_ingredients.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_syn = nx.Graph()\n",
    "ingredients_root = 'data/ingredients/'\n",
    "for e in os.listdir(ingredients_root):\n",
    "    file_path = ingredients_root + e\n",
    "    if os.path.isfile(file_path):\n",
    "        if is_spanish_ingredients_file(e):\n",
    "            with open(file_path) as f:\n",
    "                for line in f:\n",
    "                    syn_set = set()\n",
    "                    ingrs1 = line.strip()\n",
    "                    for ingrs2 in ingrs1.split(' / '):\n",
    "                        for ingrs3 in ingrs2.split(' o '):\n",
    "                            for ingr in ingrs3.split(' - '):\n",
    "                                syn_set.add(ingr)\n",
    "#                                 is_elbulli = e == 'es_elbulli_ingredients.txt'\n",
    "                                if not ingr in graph_syn:\n",
    "                                    graph_syn.add_node(ingr, count=1)\n",
    "#                                     graph_syn.node[ingr]['elbulli'] = is_elbulli\n",
    "                                else:\n",
    "                                    graph_syn.node[ingr]['count'] += 1\n",
    "#                                     graph_syn.node[ingr]['elbulli'] = graph_syn.node[ingr]['elbulli'] or is_elbulli\n",
    "                    syn_set = list(syn_set)\n",
    "                    i1 = syn_set[0]\n",
    "                    for i2 in syn_set[1:]:\n",
    "                        graph_syn.add_edge(i1, i2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3322"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3221"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.number_connected_components(graph_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-243-72ea9f19d102>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\nfor n in list(graph_syn.nodes(data=True)):\\n    ingr = n[0]\\n    norms = normalize(ingr)\\n    for norm in norms:\\n        if not norm in graph_syn:\\n            graph_syn.add_node(norm, count=1)\\n        else:\\n            graph_syn.node[norm]['count'] += 1\\n        graph_syn.add_edge(ingr, norm)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/antonio/virtualenvs/elbulli/lib/python3.4/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2291\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2292\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2293\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2294\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32m/home/antonio/virtualenvs/elbulli/lib/python3.4/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/antonio/virtualenvs/elbulli/lib/python3.4/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1165\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1166\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1167\u001b[1;33m             \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1168\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1169\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-118-90bb9b18dca2>\u001b[0m in \u001b[0;36mnormalize\u001b[1;34m(ingredient)\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m                 \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-118-90bb9b18dca2>\u001b[0m in \u001b[0;36msingular\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msingular\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[0mfuncs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msingular\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_tags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccent_marks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnonascii_consonants\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdashes1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdashes2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-126-21f1ea05e0e0>\u001b[0m in \u001b[0;36mlemmatize\u001b[1;34m(ingredient)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mingredient\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mingredient\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msingularize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-126-21f1ea05e0e0>\u001b[0m in \u001b[0;36msingularize\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0msing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'dos'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mes_lexicon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'flexion'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mhas_category\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'noun'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/antonio/virtualenvs/elbulli/lib/python3.4/site-packages/pymongo/cursor.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1095\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1096\u001b[0m         \u001b[0m_db\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__collection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1097\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__data\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_refresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1098\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__manipulate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m                 return _db._fix_outgoing(self.__data.popleft(),\n",
      "\u001b[1;32m/home/antonio/virtualenvs/elbulli/lib/python3.4/site-packages/pymongo/cursor.py\u001b[0m in \u001b[0;36m_refresh\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1017\u001b[0m                                        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__limit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1018\u001b[0m                                        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__batch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1019\u001b[1;33m                                        self.__read_concern))\n\u001b[0m\u001b[0;32m   1020\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__id\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__killed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/antonio/virtualenvs/elbulli/lib/python3.4/site-packages/pymongo/cursor.py\u001b[0m in \u001b[0;36m__send_message\u001b[1;34m(self, operation)\u001b[0m\n\u001b[0;32m    848\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m                 response = client._send_message_with_response(operation,\n\u001b[1;32m--> 850\u001b[1;33m                                                               **kwargs)\n\u001b[0m\u001b[0;32m    851\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__address\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    852\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__exhaust\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/antonio/virtualenvs/elbulli/lib/python3.4/site-packages/pymongo/mongo_client.py\u001b[0m in \u001b[0;36m_send_message_with_response\u001b[1;34m(self, operation, read_preference, exhaust, address)\u001b[0m\n\u001b[0;32m    792\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__all_credentials\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event_listeners\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 794\u001b[1;33m             exhaust)\n\u001b[0m\u001b[0;32m    795\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_reset_on_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/antonio/virtualenvs/elbulli/lib/python3.4/site-packages/pymongo/mongo_client.py\u001b[0m in \u001b[0;36m_reset_on_error\u001b[1;34m(self, server, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    803\u001b[0m         \"\"\"\n\u001b[0;32m    804\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 805\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    806\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mNetworkTimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[1;31m# The socket has been closed. Don't reset the server.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/antonio/virtualenvs/elbulli/lib/python3.4/site-packages/pymongo/server.py\u001b[0m in \u001b[0;36msend_message_with_response\u001b[1;34m(self, operation, set_slave_okay, all_credentials, listeners, exhaust)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[0msock_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_doc_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m                 \u001b[0mresponse_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msock_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreceive_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mpublish\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/antonio/virtualenvs/elbulli/lib/python3.4/site-packages/pymongo/pool.py\u001b[0m in \u001b[0;36mreceive_message\u001b[1;34m(self, operation, request_id)\u001b[0m\n\u001b[0;32m    244\u001b[0m                 self.sock, operation, request_id, self.max_message_size)\n\u001b[0;32m    245\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 246\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_connection_failure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlegacy_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_doc_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwith_last_error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/antonio/virtualenvs/elbulli/lib/python3.4/site-packages/pymongo/pool.py\u001b[0m in \u001b[0;36m_raise_connection_failure\u001b[1;34m(self, error)\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[0m_raise_connection_failure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/antonio/virtualenvs/elbulli/lib/python3.4/site-packages/pymongo/pool.py\u001b[0m in \u001b[0;36mreceive_message\u001b[1;34m(self, operation, request_id)\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m             return receive_message(\n\u001b[1;32m--> 244\u001b[1;33m                 self.sock, operation, request_id, self.max_message_size)\n\u001b[0m\u001b[0;32m    245\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_connection_failure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/antonio/virtualenvs/elbulli/lib/python3.4/site-packages/pymongo/network.py\u001b[0m in \u001b[0;36mreceive_message\u001b[1;34m(sock, operation, request_id, max_message_size)\u001b[0m\n\u001b[0;32m    120\u001b[0m         sock, operation, request_id, max_message_size=MAX_MESSAGE_SIZE):\n\u001b[0;32m    121\u001b[0m     \u001b[1;34m\"\"\"Receive a raw BSON message or raise socket.error.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m     \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_receive_data_on_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m     \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_UNPACK_INT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/antonio/virtualenvs/elbulli/lib/python3.4/site-packages/pymongo/network.py\u001b[0m in \u001b[0;36m_receive_data_on_socket\u001b[1;34m(sock, length)\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m             \u001b[0mchunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mIOError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "count = 0\n",
    "for n in list(graph_syn.nodes(data=True)):\n",
    "    ingr = n[0]\n",
    "    norms = normalize(ingr)\n",
    "    for norm in norms:\n",
    "        if not norm in graph_syn:\n",
    "            graph_syn.add_node(norm, count=1)\n",
    "        else:\n",
    "            graph_syn.node[norm]['count'] += 1\n",
    "        graph_syn.add_edge(ingr, norm)\n",
    "    \n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(count, 'nodes processed.')\n",
    "if count % 1000 == 0:\n",
    "    print(count, 'nodes processed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7523"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3094"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.number_connected_components(graph_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def my_ngrams(ingredient):\n",
    "    ngrms = []\n",
    "    tokens = nltk.word_tokenize(ingredient)\n",
    "    for i in range(1, len(tokens) + 1):\n",
    "        ngrms.extend(ngrams(tokens, i))\n",
    "    return list(map(lambda x: ' '.join(x), ngrms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lengths = defaultdict(int)\n",
    "for ingr in graph_syn.nodes_iter():\n",
    "    lengths[len(nltk.word_tokenize(ingr))] += 1\n",
    "lengths = dict(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1209, 2: 2547, 3: 2552, 4: 699, 5: 334, 6: 119, 7: 28, 8: 16, 9: 15, 10: 4}"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arroz', 'kome'}"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.node_connected_component(graph_syn, 'arroz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 nodes processed.\n",
      "200 nodes processed.\n",
      "300 nodes processed.\n",
      "400 nodes processed.\n",
      "500 nodes processed.\n",
      "600 nodes processed.\n",
      "700 nodes processed.\n",
      "800 nodes processed.\n",
      "900 nodes processed.\n",
      "1000 nodes processed.\n",
      "1100 nodes processed.\n",
      "1200 nodes processed.\n",
      "1300 nodes processed.\n",
      "1400 nodes processed.\n",
      "1500 nodes processed.\n",
      "1600 nodes processed.\n",
      "1700 nodes processed.\n",
      "1800 nodes processed.\n",
      "1900 nodes processed.\n",
      "2000 nodes processed.\n",
      "2100 nodes processed.\n",
      "2200 nodes processed.\n",
      "2300 nodes processed.\n",
      "2400 nodes processed.\n",
      "2500 nodes processed.\n",
      "2600 nodes processed.\n",
      "2700 nodes processed.\n",
      "2800 nodes processed.\n",
      "2900 nodes processed.\n",
      "3000 nodes processed.\n",
      "3100 nodes processed.\n",
      "3200 nodes processed.\n",
      "3300 nodes processed.\n",
      "3400 nodes processed.\n",
      "3500 nodes processed.\n",
      "3600 nodes processed.\n",
      "3700 nodes processed.\n",
      "3800 nodes processed.\n",
      "3900 nodes processed.\n",
      "4000 nodes processed.\n",
      "4100 nodes processed.\n",
      "4200 nodes processed.\n",
      "4300 nodes processed.\n",
      "4400 nodes processed.\n",
      "4500 nodes processed.\n",
      "4600 nodes processed.\n",
      "4700 nodes processed.\n",
      "4800 nodes processed.\n",
      "4900 nodes processed.\n",
      "5000 nodes processed.\n",
      "5100 nodes processed.\n",
      "5200 nodes processed.\n",
      "5300 nodes processed.\n",
      "5400 nodes processed.\n",
      "5500 nodes processed.\n",
      "5600 nodes processed.\n",
      "5700 nodes processed.\n",
      "5800 nodes processed.\n",
      "5900 nodes processed.\n",
      "6000 nodes processed.\n",
      "6100 nodes processed.\n",
      "6200 nodes processed.\n",
      "6300 nodes processed.\n",
      "6400 nodes processed.\n",
      "6500 nodes processed.\n",
      "6600 nodes processed.\n",
      "6700 nodes processed.\n",
      "6800 nodes processed.\n",
      "6900 nodes processed.\n",
      "7000 nodes processed.\n",
      "7100 nodes processed.\n",
      "7200 nodes processed.\n",
      "7300 nodes processed.\n",
      "7400 nodes processed.\n",
      "7500 nodes processed.\n",
      "CPU times: user 1.78 s, sys: 4 ms, total: 1.78 s\n",
      "Wall time: 1.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "count = 0\n",
    "for n in list(graph_syn.nodes(data=True)):\n",
    "    ingr = n[0]\n",
    "    ngrms = my_ngrams(ingr)\n",
    "    for ngrm in ngrms:\n",
    "        if ngrm in graph_syn:\n",
    "            syns = nx.node_connected_component(graph_syn, ngrm)\n",
    "            for syn in syns:\n",
    "                syn_ingr = ingr.replace(ngrm, syn)\n",
    "                if not syn_ingr in graph_syn:\n",
    "                    graph_syn.add_node(syn_ingr, count=1)\n",
    "                else:\n",
    "                    graph_syn.node[syn_ingr]['count'] += 1\n",
    "                graph_syn.add_edge(ingr, syn_ingr)\n",
    "    \n",
    "    count += 1\n",
    "    if count % 100 == 0:\n",
    "        print(count, 'nodes processed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3238"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "while True:\n",
    "    n_nodes_old = graph_syn.number_of_nodes()\n",
    "    n_edges_old = graph_syn.number_of_edges()\n",
    "    for n in list(graph_syn.nodes(data=True)):\n",
    "        norms = normalize(n)\n",
    "        for norm in norms:\n",
    "            \n",
    "        ngrms = my_ngrams(n)\n",
    "        if len(tokens) == length:\n",
    "            norms = normalize(n)\n",
    "            if not lemmatized in graph_syn:\n",
    "                graph_syn.add_node(lemmatized, count=1)\n",
    "            else:\n",
    "                graph_syn.node[lemmatized]['count'] += 1\n",
    "            graph_syn.add_edge(ingr, lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'salsa de soja', 'shoyu', 'shō-yu', 'shōyu'}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.node_connected_component(graph_syn, 'salsa de soja')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for line in all_lines:\n",
    "    syn_set = set()\n",
    "    ingrs1 = line.strip()\n",
    "    for ingrs2 in ingrs1.split(' / '):\n",
    "        for ingr in ingrs2.split(' o '):\n",
    "            syn_set.add(ingr)\n",
    "    syn_set = list(syn_set)\n",
    "    i1 = syn_set[0]\n",
    "    for i2 in syn_set[1:]:\n",
    "        graph_syn.add_edge(i1, i2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nonascii = set()\n",
    "for ingr in graph_syn.nodes_iter():\n",
    "#     q = ingr.replace(' ', 'xxx')\n",
    "#     if not q.isalpha():\n",
    "#         print(q.replace('xxx', ' '))\n",
    "    if not is_ascii(ingr):\n",
    "        for c in ingr:\n",
    "            if not is_ascii(c):\n",
    "                nonascii.add(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(expr):\n",
    "    pass\n",
    "\n",
    "def lemmatize(expr):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for line in all_lines:\n",
    "    syn_set = set()\n",
    "    ingrs1 = line.strip()\n",
    "    for ingrs2 in ingrs1.split(' / '):\n",
    "        for ingr in ingrs2.split(' o '):\n",
    "            lemmatized = lemmatize(ingr)\n",
    "            if not lemmatized in graph_syn:\n",
    "                graph_syn.add_node(lemmatized, count=1, is_lemma=True, is_repr=False)\n",
    "            else:\n",
    "                graph_syn.node[lemmatized]['count'] += 1\n",
    "                graph_syn.node[lemmatized]['is_lemma'] = True\n",
    "            syn_set.add(ingr)\n",
    "            syn_set.add(lemmatized)\n",
    "    syn_set = list(syn_set)\n",
    "    i1 = syn_set[0]\n",
    "    for i2 in syn_set[1:]:\n",
    "        graph_syn.add_edge(i1, i2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g=nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g.add_nodes_from([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding 2\n",
      "adding 3\n",
      "adding 4\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "for x in list(g.nodes()):\n",
    "    g.add_node(x+1)\n",
    "    print('adding',x+1)\n",
    "    c+=1\n",
    "    if c>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g.add_edge(1, 1, b=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1, {'b': 5})]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.edges(data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g.add_node(1, {'a':3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, {'a': 3})]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.nodes(data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g.node[1]['a'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.probability.FreqDist"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nltk.FreqDist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmatizer(x):\n",
    "    return nltk.FreqDist(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xxx=lemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_es_ingredient(ingredient):\n",
    "    #ingredient tiene format \"salsa de soja o salsa de soya\" -> esto no puede ir en las funciones clean\n",
    "    #pq entonces se pierde la info de que son ingredientes sinonimos\n",
    "    #puede estar en singular o plurar\n",
    "    #hay que lematizar la expresion (ingredient) completa, eliminar preps, arts... segun mi criterio\n",
    "    #hay que guardar todas las posibles variantes del ingredient y su lematizacion\n",
    "    #guardar esta funcion en un obj pickle y \"exportarla\" donde sea necesario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    with open('pickle/lemmatizer.pickle', 'wb') as f:\n",
    "        pickle.dump(xxx, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class A:\n",
    "    def lemmatizer(self, x):\n",
    "        return nltk.FreqDist(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xxx = A()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'a': 2, 'b': 1})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xxx.lemmatizer(['a','a','b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aceite']\n",
      "['óleo']\n",
      "['petróleo']\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    ">>> from nltk.corpus import wordnet as wn\n",
    "for ss in wn.synsets('oil'):\n",
    "    print(ss.lemma_names('spa'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method WordNetCorpusReader.langs of <WordNetCorpusReader in '/home/antonio/nltk_data/corpora/wordnet'>>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.langs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'omw'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-77520f454251>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0momw\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'omw'"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import omw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l=wn.lemmas('cane', lang='ita')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('dog.n.01')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.synset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lemma('dog.n.01.cane')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls = wn.lemmas('amaba', lang='spa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for x in ls:\n",
    "    print(x.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lematizar solamente usando mi lexicon en español\n",
    "si no existe la palabra y no se puede lematizar, quitar s final si existe\n",
    "esto aplica a adj y noun, poco probable encontrar un verbo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
