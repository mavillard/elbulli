{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shlex\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import cess_esp\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagged_sp_sents = cess_esp.tagged_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = int(len(tagged_sp_sents) * 0.1)\n",
    "train_sp_sents = tagged_sp_sents[size:]\n",
    "test_sp_sents = tagged_sp_sents[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tagged_sp_sents) == len(train_sp_sents) + len(test_sp_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_sp_words = cess_esp.tagged_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sps00', 25272),\n",
       " ('ncms000', 11428),\n",
       " ('Fc', 11420),\n",
       " ('ncfs000', 11008),\n",
       " ('da0fs0', 6838),\n",
       " ('da0ms0', 6012),\n",
       " ('rg', 5937),\n",
       " ('Fp', 5866),\n",
       " ('cc', 5854),\n",
       " ('ncmp000', 5711)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = [tag for (word, tag) in tagged_sp_words]\n",
    "most_freq_tags = nltk.FreqDist(tags)\n",
    "most_freq_tags.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "default_tag = 'ncms000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t0 = nltk.DefaultTagger(default_tag)\n",
    "t1 = nltk.UnigramTagger(train_sp_sents, backoff=t0)\n",
    "t2 = nltk.BigramTagger(train_sp_sents, backoff=t1)\n",
    "sp_tagger = nltk.TrigramTagger(train_sp_sents, backoff=t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8843257443082312"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_tagger.evaluate(test_sp_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# states\n",
    "# 'NODETYPES', 'EDGETYPES', 'NODE', 'NODEFIELDS', 'EDGES', 'EDGEFIELDS', 'NOTHING'\n",
    "current_st = 'NOTHING'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ingredients = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_db = 'data/BulliCompletoEditado.nlg'\n",
    "with open(path_db) as f:\n",
    "    for line in f:\n",
    "        stripped_line = line.strip()\n",
    "        if stripped_line == '\"<NodesTypes>\"' and current_st == 'NOTHING':\n",
    "            current_st = 'NODETYPES'\n",
    "        elif stripped_line == '\"<EndNodesTypes>\"' and current_st == 'NODETYPES':\n",
    "            current_st = 'NOTHING'\n",
    "        elif stripped_line == '\"<EdgesTypes>\"' and current_st == 'NOTHING':\n",
    "            current_st = 'EDGETYPES'\n",
    "        elif stripped_line == '\"<EndEdgesTypes>\"' and current_st == 'EDGETYPES':\n",
    "            current_st = 'NOTHING'\n",
    "        elif stripped_line == '\"<Nodes>\"' and current_st == 'NOTHING':\n",
    "            current_st = 'NODEFIELDS'\n",
    "        elif stripped_line == '\"<EndNodes>\"' and current_st == 'NODE':\n",
    "            current_st = 'NOTHING'\n",
    "        elif stripped_line == '\"<Edges>\"' and current_st == 'NOTHING':\n",
    "            current_st = 'EDGEFIELDS'\n",
    "        elif stripped_line == '\"<EndEdges>\"' and current_st == 'EDGE':\n",
    "            current_st = 'NOTHING'\n",
    "        elif current_st == 'NODETYPES':\n",
    "            pass\n",
    "        elif current_st == 'EDGETYPES':\n",
    "            pass\n",
    "        elif current_st == 'NODEFIELDS':\n",
    "            current_st = 'NODE'\n",
    "        elif current_st == 'EDGEFIELDS':\n",
    "            current_st = 'EDGE'\n",
    "        elif current_st == 'NODE':\n",
    "            if stripped_line.endswith('\"sabor\"'):\n",
    "                values = shlex.split(stripped_line)\n",
    "                ide = values[0]\n",
    "                name = ide[len('sabor:'):]\n",
    "                exprs = name.split(' / ')\n",
    "                for expr in exprs:\n",
    "                    ingrs = expr.split(' y ')\n",
    "                    if len(ingrs) == 1:\n",
    "                        ingredients.add(ingrs[0])\n",
    "                    else:\n",
    "                        for ingr in ingrs:\n",
    "                            tokens = nltk.word_tokenize(ingr)\n",
    "                            tags = sp_tagger.tag(tokens)\n",
    "                            if any(map(lambda x: x[1].startswith('n'), tags)):\n",
    "                                ingredients.add(ingr)\n",
    "            elif stripped_line.endswith('\"ingrediente\"'):\n",
    "                values = shlex.split(stripped_line)\n",
    "                ide = values[0]\n",
    "                name = ide\n",
    "                exprs1 = [n.strip() for n in name.split(',')]\n",
    "                for expr1 in exprs1:\n",
    "                    exprs2 = expr1.split(' / ')\n",
    "                    for expr2 in exprs2:\n",
    "                        ingrs = expr2.split(' y ')\n",
    "                        if len(ingrs) == 1:\n",
    "                            ingredients.add(ingrs[0])\n",
    "                        else:\n",
    "                            for ingr in ingrs:\n",
    "                                tokens = nltk.word_tokenize(ingr)\n",
    "                                tags = sp_tagger.tag(tokens)\n",
    "                                if any(map(lambda x: x[1].startswith('n'), tags)):\n",
    "                                    ingredients.add(ingr)\n",
    "        elif current_st == 'EDGE':\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1922"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ingredients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "client.drop_database('sp_lexicon')\n",
    "db = client.sp_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 s, sys: 64 ms, total: 12.1 s\n",
      "Wall time: 18.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('data/sp_lexicon.csv') as f:\n",
    "    reader = csv.reader(\n",
    "        f,\n",
    "        delimiter=' ',\n",
    "    )\n",
    "    docs = []\n",
    "    count = 0\n",
    "    for row in reader:\n",
    "        for i in range(1, len(row[1:]), 2):\n",
    "            entry = {}\n",
    "            entry['flexion'] = row[0]\n",
    "            entry['lemma'] = row[i]\n",
    "            entry['eagle'] = row[i+1]\n",
    "            docs.append(entry)\n",
    "            count += 1\n",
    "        if count % 1000 == 0:\n",
    "            db.lexicon.insert_many(docs)\n",
    "            docs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "668000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.lexicon.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def singularize(word):\n",
    "#     amarilla amarillo AQ0FS0\n",
    "#     amarillas amarillo AQ0FP0\n",
    "    singular de \"amarillas\" es la flexion cuyo lemma sea el mismo que el de \"amarillas\" pero cuyo eagle sea S en vez de P\n",
    "    adj y nouns o tb articulos?\n",
    "    return result\n",
    "\n",
    "\n",
    "extraer los ingredientes del texto de la preparacion o sacarlos del grafo que ya tengo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "singularized_ingredients = set()\n",
    "for \n",
    "            singularized = ' '.join(map(singularize, ingredient.split()))\n",
    "            ingredients.add(singularized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/ingredients/elbulli_ingredients.txt', 'w') as f:\n",
    "    f.write('\\n'.join(sorted(ingredients)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
