{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "from itertools import permutations\n",
    "\n",
    "import networkx as nx\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# No English lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pos_tagger(tokens):\n",
    "    return nltk.pos_tag(tokens, tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_english_techniques_file(filename):\n",
    "    return filename.startswith('en_') and filename.endswith('_techniques.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_node(g, n):\n",
    "    if not n in g:\n",
    "        g.add_node(n, count=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_edge(g, n1, n2):\n",
    "    if n1 != n2 and not nx.has_path(g, n1, n2):\n",
    "        g.add_edge(n1, n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_syn = nx.Graph()\n",
    "techniques_root = 'data/techniques/'\n",
    "for e in os.listdir(techniques_root):\n",
    "    file_path = techniques_root + e\n",
    "    if os.path.isfile(file_path):\n",
    "        if is_english_techniques_file(e):\n",
    "            with open(file_path) as f:\n",
    "                for line in f:\n",
    "                    syn_set = set()\n",
    "                    techs1 = line.strip()\n",
    "                    for tech in techs1.split(' or '):\n",
    "                        syn_set.add(tech)\n",
    "                        add_node(graph_syn, tech)\n",
    "                    syn_set = list(syn_set)\n",
    "                    i1 = syn_set[0]\n",
    "                    for i2 in syn_set[1:]:\n",
    "                        add_edge(graph_syn, i1, i2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_syn.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "284"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.number_connected_components(graph_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nx.write_gexf(graph_syn, 'data/english_techniques_lexicon_1.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_syn = nx.read_gexf('data/english_techniques_lexicon_1.gexf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def technique_tagger(x):\n",
    "    result = []\n",
    "    tokens = nltk.word_tokenize(x)\n",
    "    tags = pos_tagger(tokens)\n",
    "    for token, tag in tags:\n",
    "        result.append((token, tag.lower()))\n",
    "    if len(result) == 1:\n",
    "        tag = result[0][1]\n",
    "        if tag not in ['noun', 'verb']:\n",
    "            tag = 'noun'\n",
    "        result = [(x, tag)]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 352 ms, sys: 20 ms, total: 372 ms\n",
      "Wall time: 369 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with open('data/english_techniques_postags.csv', 'w') as f:\n",
    "    writer = csv.writer(\n",
    "        f,\n",
    "        delimiter=',',\n",
    "        quotechar='\"',\n",
    "        quoting=csv.QUOTE_MINIMAL\n",
    "    )\n",
    "    for tech in graph_syn.nodes_iter():\n",
    "        pos_tag = ' '.join(tag for token, tag in technique_tagger(tech))\n",
    "        row = [tech, pos_tag]\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "postags = {}\n",
    "with open('data/english_techniques_postags.csv') as f:\n",
    "    reader = csv.reader(\n",
    "        f,\n",
    "        delimiter=',',\n",
    "    )\n",
    "    for row in reader:\n",
    "        postags[row[0]] = row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('deep', 'adj'), ('fry', 'noun')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_postags(x):\n",
    "    try:\n",
    "        tags = postags[x]\n",
    "    except:\n",
    "        postags[x] = ' '.join(tag for token, tag in technique_tagger(x))\n",
    "        tags = postags[x]\n",
    "    return list(zip(nltk.word_tokenize(x),nltk.word_tokenize(tags)))\n",
    "\n",
    "# Example\n",
    "get_postags('deep fry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wordnet synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nouns_and_verbs = set()\n",
    "for tech in graph_syn.nodes_iter():\n",
    "    tags = get_postags(tech)\n",
    "    for token, tag in tags:\n",
    "        if tag in ['noun', 'verb']:\n",
    "            nouns_and_verbs.add(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "368"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nouns_and_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.51 s, sys: 64 ms, total: 2.58 s\n",
      "Wall time: 2.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with open('data/wordnet_techniques_synonyms.csv', 'w') as f:\n",
    "    writer = csv.writer(\n",
    "        f,\n",
    "        delimiter=',',\n",
    "        quotechar='\"',\n",
    "        quoting=csv.QUOTE_MINIMAL\n",
    "    )\n",
    "    for x in nouns_and_verbs:\n",
    "        if x in graph_syn:\n",
    "            syns = []\n",
    "            for ss in wn.synsets(x):\n",
    "                if ss.name().startswith(x) and ss.pos() in ['n', 'v']:\n",
    "                    syns.extend(map(lambda y: y.replace('_', ' ').lower(), ss.lemma_names()))\n",
    "            if syns:\n",
    "                row = [x] + syns\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordnet_syns = {}\n",
    "with open('data/wordnet_techniques_synonyms.csv') as f:\n",
    "    reader = csv.reader(\n",
    "        f,\n",
    "        delimiter=',',\n",
    "    )\n",
    "    for row in reader:\n",
    "        wordnet_syns[row[0]] = row[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordnet_syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordnet_graph = nx.Graph()\n",
    "for k in wordnet_syns:\n",
    "    syns = wordnet_syns[k]\n",
    "    for syn in syns:\n",
    "        wordnet_graph.add_edge(k, syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.number_connected_components(wordnet_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_kn_complete(g):\n",
    "    complete = True\n",
    "    for n1 in g:\n",
    "        for n2 in g:\n",
    "            if n1 != n2 and not g.has_edge(n1, n2):\n",
    "                complete = False\n",
    "                break\n",
    "        if not complete:\n",
    "            break\n",
    "    return complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kn_complete_graphs = []\n",
    "for subg in nx.connected_component_subgraphs(wordnet_graph):\n",
    "    if is_kn_complete(subg):\n",
    "        kn_complete_graphs.append(subg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kn_complete_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syns_found = 0\n",
    "for g in kn_complete_graphs:\n",
    "    syn_set = g.nodes()\n",
    "    i1 = syn_set[0]\n",
    "    add_node(graph_syn, i1)\n",
    "    for i2 in syn_set[1:]:\n",
    "        add_node(graph_syn, i2)\n",
    "        add_edge(graph_syn, i1, i2)\n",
    "        syns_found += 1\n",
    "syns_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "424"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_syn.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "284"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.number_connected_components(graph_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nx.write_gexf(graph_syn, 'data/english_techniques_lexicon_2.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_syn = nx.read_gexf('data/english_techniques_lexicon_2.gexf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Infinitive, gerund, participle, and noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deep fry'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_infinitivable_word(word_tag):\n",
    "    tag = word_tag[1]\n",
    "    return tag in ['noun', 'verb']\n",
    "\n",
    "def is_infinitivable_technique(technique):\n",
    "    return any(map(is_infinitivable_word, get_postags(technique)))\n",
    "\n",
    "def naive_infinitive_noun(word):\n",
    "    return word\n",
    "\n",
    "def infinitive_verb(word):\n",
    "    return lemmatizer.lemmatize(word, pos='v')\n",
    "\n",
    "def infinitive_word(word_tag):\n",
    "    word = word_tag[0]\n",
    "    tag = word_tag[1]\n",
    "    inf = word\n",
    "    if word.isalpha():\n",
    "        if tag == 'noun':\n",
    "            inf = naive_infinitive_noun(word)\n",
    "        elif tag == 'verb':\n",
    "            inf = infinitive_verb(word)\n",
    "    return inf\n",
    "\n",
    "def infinitive_technique(technique):\n",
    "    infs = map(infinitive_word, get_postags(technique))\n",
    "    return ' '.join(infs)\n",
    "\n",
    "# Example\n",
    "infinitive_technique('deep fried')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deep frying'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_gerundable_word(word_tag):\n",
    "    tag = word_tag[1]\n",
    "    return tag in ['noun', 'verb']\n",
    "\n",
    "def is_gerundable_technique(technique):\n",
    "    return any(map(is_gerundable_word, get_postags(technique)))\n",
    "\n",
    "def naive_gerund_noun(word):\n",
    "    lemma = lemmatizer.lemmatize(word)\n",
    "    ger = lemma + 'ing'\n",
    "    if ger not in words.words():\n",
    "        ger = word\n",
    "    return ger\n",
    "\n",
    "def gerund_verb(word):\n",
    "    if word.endswith('ing'):\n",
    "        return word\n",
    "    \n",
    "    def count_vowels(s):\n",
    "        c = 0\n",
    "        for v in 'aeiou':\n",
    "            c += s.count(v)\n",
    "        return c\n",
    "    \n",
    "    def is_vowel(x):\n",
    "        return x in 'aeiou'\n",
    "    \n",
    "    def is_consonant(x):\n",
    "        return not is_vowel(x)\n",
    "    \n",
    "    lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "    ger = lemma + 'ing'\n",
    "    if lemma.endswith('ie'): # lie -> lying\n",
    "        ger = lemma[:-2] + 'ying'\n",
    "    elif lemma.endswith('e') and lemma != 'be': # make -> making\n",
    "        ger = lemma[:-1] + 'ing'\n",
    "    elif len(lemma) >= 3 and is_consonant(word[-3]) and is_vowel(word[-2]) and is_consonant(word[-1]):\n",
    "        if word[-1] not in 'wxy':\n",
    "            if count_vowels(word) == 1: # sit -> sitting\n",
    "                ger = lemma + lemma[-1] + 'ing'\n",
    "    return ger\n",
    "\n",
    "def gerund_word(word_tag):\n",
    "    word = word_tag[0]\n",
    "    tag = word_tag[1]\n",
    "    ger = word\n",
    "    if word.isalpha():\n",
    "        if tag == 'noun':\n",
    "            ger = naive_gerund_noun(word)\n",
    "        elif tag == 'verb':\n",
    "            ger = gerund_verb(word)\n",
    "    return ger\n",
    "\n",
    "def gerund_technique(technique):\n",
    "    gers = map(gerund_word, get_postags(technique))\n",
    "    return ' '.join(gers)\n",
    "\n",
    "# Example\n",
    "gerund_technique('deep fried')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deep fry'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_participlable_word(word_tag):\n",
    "    tag = word_tag[1]\n",
    "    return tag in ['noun', 'verb']\n",
    "\n",
    "def is_participlable_technique(technique):\n",
    "    return any(map(is_participlable_word, get_postags(technique)))\n",
    "\n",
    "def naive_participle_noun(word):\n",
    "    lemma = lemmatizer.lemmatize(word)\n",
    "    par = lemma + 'ed'\n",
    "    if par not in words.words():\n",
    "        par = word\n",
    "    return par\n",
    "\n",
    "def participle_verb(word):\n",
    "    if word.endswith('ed'):\n",
    "        return word\n",
    "    \n",
    "    def count_vowels(s):\n",
    "        c = 0\n",
    "        for v in 'aeiou':\n",
    "            c += s.count(v)\n",
    "        return c\n",
    "    \n",
    "    def is_vowel(x):\n",
    "        return x in 'aeiou'\n",
    "    \n",
    "    def is_consonant(x):\n",
    "        return not is_vowel(x)\n",
    "    \n",
    "    lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "    par = lemma + 'ed'\n",
    "    if lemma.endswith('e'): # live -> lived\n",
    "        par = lemma + 'd'\n",
    "    elif len(lemma) >= 3 and is_consonant(word[-3]) and is_vowel(word[-2]) and is_consonant(word[-1]):\n",
    "        if word[-1] not in 'wxy':\n",
    "            if count_vowels(word) == 1: # stop -> stopped\n",
    "                par = lemma + lemma[-1] + 'ed'\n",
    "    if par not in words.words():\n",
    "        par = word\n",
    "    return par\n",
    "\n",
    "def participle_word(word_tag):\n",
    "    word = word_tag[0]\n",
    "    tag = word_tag[1]\n",
    "    par = word\n",
    "    if word.isalpha():\n",
    "        if tag == 'noun':\n",
    "            par = naive_participle_noun(word)\n",
    "        elif tag == 'verb':\n",
    "            par = participle_verb(word)\n",
    "    return par\n",
    "\n",
    "def participle_technique(technique):\n",
    "    pars = map(participle_word, get_postags(technique))\n",
    "    return ' '.join(pars)\n",
    "\n",
    "# Example\n",
    "participle_technique('deep fry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.6 s, sys: 5.32 s, total: 41.9 s\n",
      "Wall time: 41.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for tech in graph_syn.nodes():\n",
    "    if is_infinitivable_technique(tech):\n",
    "        inf = infinitive_technique(tech)\n",
    "        add_node(graph_syn, inf)\n",
    "        add_edge(graph_syn, tech, inf)\n",
    "    if is_gerundable_technique(tech):\n",
    "        ger = gerund_technique(tech)\n",
    "        add_node(graph_syn, ger)\n",
    "        add_edge(graph_syn, tech, ger)\n",
    "    if is_participlable_technique(tech):\n",
    "        par = participle_technique(tech)\n",
    "        add_node(graph_syn, par)\n",
    "        add_edge(graph_syn, tech, par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "582"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_syn.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.number_connected_components(graph_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nx.write_gexf(graph_syn, 'data/english_techniques_lexicon_3.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_syn = nx.read_gexf('data/english_techniques_lexicon_3.gexf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Numbers\n",
    "def numbers(x):\n",
    "    return x.replace(' 1 ', ' one ') \\\n",
    "            .replace(' 2 ', ' two ') \\\n",
    "            .replace(' 3 ', ' three ') \\\n",
    "            .replace(' 4 ', ' four ') \\\n",
    "            .replace(' 5 ', ' five ') \\\n",
    "            .replace(' 6 ', ' six ') \\\n",
    "            .replace(' 7 ', ' seven ') \\\n",
    "            .replace(' 8 ', ' eight ') \\\n",
    "            .replace(' 9 ', ' nine ')\n",
    "    \n",
    "# Dashes (-)\n",
    "def dashes1(x):\n",
    "    return x.replace('-', ' ')\n",
    "\n",
    "def dashes2(x):\n",
    "    return x.replace('-', '')\n",
    "\n",
    "# POS tags\n",
    "# ADJETIVOS .... A ADJ ...... X\n",
    "# ADVERBIOS .... R ADV\n",
    "# DETERMINANTES  D DET\n",
    "# NOMBRES ...... N NOUN ..... X\n",
    "# VERBOS ....... V VERB ..... X\n",
    "# PRONOMBRES ... P PRON\n",
    "# CONJUNCIONES . C CONJ\n",
    "# INTERJECCIONES I INTERJ\n",
    "# PREPOSICIONES  S PREP\n",
    "# PUNTUACIÓN ... F PUNTUATION\n",
    "# NUMERALES .... Z NUM ...... X\n",
    "# FECHAS Y HORAS W DATE-TIME\n",
    "def pos_tags(x):\n",
    "    tags = get_postags(x)\n",
    "    filtered = [token\n",
    "                for token, tag in tags\n",
    "                if tag in ['adj', 'noun', 'verb', 'num']\n",
    "               ]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "def itself(x):\n",
    "    return x\n",
    "\n",
    "funcs = [itself, pos_tags, numbers, dashes1, dashes2]\n",
    "combinations = []\n",
    "for i in range(1, len(funcs) + 1):\n",
    "    combinations.append(list(itertools.combinations(funcs, i)))\n",
    "combinations = [c for comb in combinations for c in comb]\n",
    "\n",
    "# def normalize(technique): # original time consuming version\n",
    "#     result = set()\n",
    "#     for c in combinations:\n",
    "#         x = technique\n",
    "#         for f in c:\n",
    "#             x = f(x)\n",
    "#         result.add(x)\n",
    "#     return result\n",
    "\n",
    "def normalize(technique): # dynamic programming version\n",
    "    result = set()\n",
    "    for c in combinations:\n",
    "        x = technique\n",
    "        for f in c:\n",
    "            if not x in d[f.__name__]:\n",
    "                d[f.__name__][x] = f(x)\n",
    "            x = d[f.__name__][x]\n",
    "        result.add(x)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([list(map(lambda x: x.__name__, c)) for c in combinations])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = defaultdict(dict)\n",
    "\n",
    "# or\n",
    "\n",
    "# with open('data/english_techniques_normalization.pickle', 'rb') as f:\n",
    "#     d = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 308 ms, sys: 0 ns, total: 308 ms\n",
      "Wall time: 309 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for tech in graph_syn.nodes():\n",
    "    if len(nltk.word_tokenize(tech)) < 4:\n",
    "        norms = normalize(tech)\n",
    "        for norm in norms:\n",
    "            add_node(graph_syn, norm)\n",
    "            add_edge(graph_syn, tech, norm)\n",
    "\n",
    "d = dict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data/english_techniques_normalization.pickle', 'wb') as f:\n",
    "    pickle.dump(d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "655"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_syn.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.number_connected_components(graph_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nx.write_gexf(graph_syn, 'data/english_techniques_lexicon_4.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_syn = nx.read_gexf('data/english_techniques_lexicon_4.gexf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def my_ngrams(technique):\n",
    "    ngrms = []\n",
    "    tokens = nltk.word_tokenize(technique)\n",
    "    for i in range(1, len(tokens) + 1):\n",
    "        ngrms.extend(ngrams(tokens, i))\n",
    "    return list(map(lambda x: ' '.join(x), ngrms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lengths = defaultdict(int)\n",
    "for tech in graph_syn.nodes_iter():\n",
    "    lengths[len(nltk.word_tokenize(tech))] += 1\n",
    "lengths = dict(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 444, 2: 170, 3: 37, 4: 4}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['green pepper'], ['green', 'pepper']]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ngram_combinations(technique):\n",
    "    combs = []\n",
    "    ngram_list = my_ngrams(technique)\n",
    "    for i in range(1, len(ngram_list) + 1):\n",
    "        combs.extend(permutations(ngram_list, i))\n",
    "    combs = [list(c) for c in combs if ' '.join(c) == technique]\n",
    "    return combs\n",
    "\n",
    "# Example\n",
    "ngram_combinations('green pepper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['green pepper', 'green peppers']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def comb_syns(expr, syn_dict):\n",
    "    res = set()\n",
    "    combs = ngram_combinations(expr)\n",
    "    for ngrms in combs:\n",
    "        syn_list = [syn_dict[ngrm] for ngrm in ngrms]\n",
    "        syn_comb = list(product(*syn_list))\n",
    "        for sc in syn_comb:\n",
    "            res.add(' '.join(sc))\n",
    "    return list(res)\n",
    "\n",
    "# Example\n",
    "expr= 'green pepper'\n",
    "syn_dict = {\n",
    "    'green': ['green'],\n",
    "    'pepper': ['pepper', 'peppers'],\n",
    "    'green pepper': ['green pepper'],\n",
    "}\n",
    "comb_syns(expr, syn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'green': {'green'}, 'green pepper': {'green pepper'}, 'pepper': {'pepper'}}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_syn_dict(ngrms):\n",
    "    d = {}\n",
    "    for ngrm in ngrms:\n",
    "        d[ngrm] = set([ngrm])\n",
    "    return d\n",
    "\n",
    "create_syn_dict(my_ngrams('green pepper'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_size_synset(synset):\n",
    "    return len(nltk.word_tokenize(sorted(synset, key=lambda x: len(nltk.word_tokenize(x)), reverse=True)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 316 ms, sys: 4 ms, total: 320 ms\n",
      "Wall time: 318 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for syns1 in list(nx.connected_components(graph_syn)):\n",
    "    max_size = max_size_synset(syns1)\n",
    "    if max_size < 4:\n",
    "        for tech in syns1:\n",
    "            ngrms = my_ngrams(tech)\n",
    "            syn_dict = create_syn_dict(ngrms)\n",
    "            for ngrm in ngrms:\n",
    "                if ngrm in graph_syn and ngrm not in syns1:\n",
    "                    syns2 = nx.node_connected_component(graph_syn, ngrm)\n",
    "                    syn_dict[ngrm] = syn_dict[ngrm].union(syns2)\n",
    "            syn_combs = comb_syns(tech, syn_dict)\n",
    "            for syn_tech in syn_combs:\n",
    "                add_node(graph_syn, syn_tech)\n",
    "                add_edge(graph_syn, tech, syn_tech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1416"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1160"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_syn.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.number_connected_components(graph_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nx.write_gexf(graph_syn, 'data/english_techniques_lexicon_5.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
