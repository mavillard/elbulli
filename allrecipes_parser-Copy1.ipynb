{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "import networkx as nx\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ingredients = []\n",
    "with open('data/ingredients/all_ingredients.txt') as f:\n",
    "    for line in f:\n",
    "        ingredient = line.strip()\n",
    "        ingredients.append(ingredient)\n",
    "ingredients = sorted(ingredients, key=len, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['high altitude adjustment for deep fat frying',\n",
       " \"mrs dash's fiesta lime salt-free seasoning\",\n",
       " 'high altitude adjustment for cookie',\n",
       " 'trappist or monastery-style cheese',\n",
       " 'high altitude baking quick bread',\n",
       " \"i can't believe it's not butter\",\n",
       " 'meat: safe cooking temperature',\n",
       " 'african birdseye chile pepper',\n",
       " 'heirloom weight & measurement',\n",
       " 'trompette de la mort mushroom']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingredients[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "techniques = []\n",
    "with open('data/techniques/all_techniques.txt') as f:\n",
    "    for line in f:\n",
    "        technique = line.strip()\n",
    "        techniques.append(technique)\n",
    "techniques = sorted(techniques, key=len, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thermal immersion circulator',\n",
       " 'extreme-heat stir-fry',\n",
       " 'flash pasteurization',\n",
       " 'amylolytic process',\n",
       " 'biomass briquettes',\n",
       " 'coagulated protein',\n",
       " 'high heat stir fry',\n",
       " 'mongolian barbecue',\n",
       " 'culinary triangle',\n",
       " 'food preservation']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "techniques[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def singularize(word):\n",
    "    if word == 'cookies':\n",
    "        result = 'cookie'\n",
    "    elif word == 'mrs':\n",
    "        result = 'mrs'\n",
    "    elif word == 'ras':\n",
    "        result = 'ras'\n",
    "    elif word == 'somen':\n",
    "        result = 'somen'\n",
    "    elif word == 'cos':\n",
    "        result = 'cos'\n",
    "    elif word == 'monks':\n",
    "        result = \"monk's\"\n",
    "    elif word == 'webbs':\n",
    "        result = \"webb's\"\n",
    "    else:\n",
    "        result = WordNetLemmatizer().lemmatize(word.lower())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inf_ing(v_ing):\n",
    "    inf = v_ing\n",
    "    if v_ing == 'de-seeding':\n",
    "        inf = 'de-seed'\n",
    "    elif v_ing == 'de-boning':\n",
    "        inf = 'de-bone'\n",
    "    elif v_ing == 'de-bearding':\n",
    "        inf = 'de-beard'\n",
    "    elif v_ing == 'gutting':\n",
    "        inf = 'gut'\n",
    "    elif v_ing == 'de-glazing':\n",
    "        inf = 'de-glaze'\n",
    "    elif v_ing == 'sautéing':\n",
    "        inf = 'sauté'\n",
    "    elif v_ing == 'degorging':\n",
    "        inf = 'degorge'\n",
    "    elif v_ing == 'stir-frying':\n",
    "        inf = 'stir-fry'\n",
    "    elif v_ing == 'broasting':\n",
    "        inf = 'broast'\n",
    "    elif v_ing == 'juicing':\n",
    "        inf = 'juice'\n",
    "    elif v_ing == 'parbaking':\n",
    "        inf = 'parbake'\n",
    "    elif v_ing == 'charbroiling':\n",
    "        inf = 'charbroil'\n",
    "    elif v_ing == 'crinkle-cutting':\n",
    "        inf = 'crinkle-cut'\n",
    "    elif v_ing == 'cheesemaking':\n",
    "        inf = 'cheesemake'\n",
    "    elif v_ing == 'swissing':\n",
    "        inf = 'swiss'\n",
    "    elif v_ing == 'flash-frying':\n",
    "        inf = 'flash-fry'\n",
    "    elif v_ing == 'flashbaking':\n",
    "        inf = 'flashbake'\n",
    "    else:\n",
    "        inf = WordNetLemmatizer().lemmatize(v_ing.lower(), 'v')\n",
    "    return inf\n",
    "\n",
    "def infinitive(words):\n",
    "    aux = []\n",
    "    word_list = words.split()\n",
    "    for word in word_list:\n",
    "        if word != 'cook' and word != 'cooking':\n",
    "            if word.endswith('ing'):\n",
    "                inf = inf_ing(word.lower())\n",
    "                aux.append(inf)\n",
    "#                 if inf == word:\n",
    "#                     print('NO INFINITIVE FOR', word)\n",
    "            else:\n",
    "                aux.append(word.lower())\n",
    "    result = ' '.join(aux)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ingr_id_graph = nx.read_gexf('data/ingredients/ingr_id_graph.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'111000'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(map(int, os.listdir(path)))[111]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'unicodeescape' codec can't decode byte 0x5c in position 30: \\ at end of string",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-edc7900a2a17>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrecipe_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data-title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf_8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'unicode_escape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'unicodeescape' codec can't decode byte 0x5c in position 30: \\ at end of string"
     ]
    }
   ],
   "source": [
    "recipe_info['data-title'][1:-2].encode('utf_8').decode('unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data/ingredients/allrecipes_ingr_ids_111.csv', 'w') as f1,\\\n",
    "     open('data/techniques/allrecipes_techniques_111.csv', 'w') as f2,\\\n",
    "     open('data/ingredient_technique_log_111.txt', 'w') as log:\n",
    "    writer1 = csv.writer(\n",
    "        f1,\n",
    "        delimiter=',',\n",
    "        quotechar='\"',\n",
    "        quoting=csv.QUOTE_MINIMAL\n",
    "    )\n",
    "    writer2 = csv.writer(\n",
    "        f2,\n",
    "        delimiter=',',\n",
    "        quotechar='\"',\n",
    "        quoting=csv.QUOTE_MINIMAL\n",
    "    )\n",
    "    writer3 = csv.writer(\n",
    "        log,\n",
    "        delimiter=',',\n",
    "        quotechar='\"',\n",
    "        quoting=csv.QUOTE_MINIMAL\n",
    "    )\n",
    "    path = '/media/antonio/WD1T/datasets allrecipes and epicurious/allrecipes/'\n",
    "#     path = 'data/allrecipes/'\n",
    "    for folder_number in sorted(map(int, os.listdir(path)))[111:]:\n",
    "        folder_name = str(folder_number)\n",
    "        folder_path = path + folder_name\n",
    "        for file_name in sorted(map(lambda x: int(x[:-5]), os.listdir(folder_path))):\n",
    "            file_name = str(file_name) + '.html'\n",
    "            file_path = folder_path + '/' + file_name\n",
    "            with open(file_path) as f:\n",
    "                soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "                recipe_info = soup.find('recipe-signup')\n",
    "                recipe_id = recipe_info['data-id']\n",
    "                recipe_title = recipe_info['data-title'][1:-1].encode('utf_8').decode('unicode_escape')\n",
    "                info_row = [recipe_id, recipe_title]\n",
    "                \n",
    "                ingredient_ids = set()\n",
    "                ingredient_amounts = soup.select('section.recipe-ingredients span[itemprop=ingredients]')\n",
    "                for ingredient_amount in ingredient_amounts:\n",
    "                    ingredient_id = ingredient_amount['data-id']\n",
    "                    content = ingredient_amount.text.strip()\n",
    "                    if content:\n",
    "                        tokens = nltk.word_tokenize(content)\n",
    "                        singularized = ' '.join(map(singularize, tokens))\n",
    "                        found_ingr = False\n",
    "                        for ingredient in ingredients:\n",
    "                            if ingredient in singularized:\n",
    "                                found_ingr = True\n",
    "                                ingredient_ids.add(ingredient_id)\n",
    "                                if ingr_id_graph.add_edge(ingredient_id, ingredient):\n",
    "                                    ingr_id_graph[ingredient_id][ingredient]['weight'] += 1\n",
    "                                else:\n",
    "                                    ingr_id_graph.add_edge(ingredient_id, ingredient, weight=1)\n",
    "                        if not found_ingr:\n",
    "                            writer3.writerow(['INGREDIENT', file_path, singularized])\n",
    "                ingredient_row = info_row + list(ingredient_ids)\n",
    "                writer1.writerow(ingredient_row)\n",
    "                \n",
    "                technique_names = set()\n",
    "                instructions = soup.select('section.recipe-directions span.recipe-directions__list--item')\n",
    "                for instruction in instructions:\n",
    "                    content = instruction.text.strip()\n",
    "                    if content:\n",
    "                        tokens = nltk.word_tokenize(content)\n",
    "                        infinitived = ' '.join(map(infinitive, tokens))\n",
    "                        found_tech = False\n",
    "                        for technique in techniques:\n",
    "                            if technique in infinitived:\n",
    "                                found_tech = True\n",
    "                                technique_names.add(technique)\n",
    "                        if not found_tech:\n",
    "                            writer3.writerow(['TECHNIQUE', file_path, infinitived])\n",
    "                technique_row = info_row + list(technique_names)\n",
    "                writer2.writerow(technique_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 recipes processed.\n",
      "20000 recipes processed.\n",
      "30000 recipes processed.\n",
      "40000 recipes processed.\n",
      "50000 recipes processed.\n",
      "60000 recipes processed.\n",
      "70000 recipes processed.\n",
      "80000 recipes processed.\n",
      "90000 recipes processed.\n",
      "100000 recipes processed.\n",
      "110000 recipes processed.\n",
      "120000 recipes processed.\n",
      "130000 recipes processed.\n",
      "140000 recipes processed.\n",
      "150000 recipes processed.\n",
      "160000 recipes processed.\n",
      "170000 recipes processed.\n",
      "180000 recipes processed.\n",
      "190000 recipes processed.\n",
      "200000 recipes processed.\n",
      "210000 recipes processed.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ingredients_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-0db9364eec09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'recipes processed.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_gexf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mingr_id_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'data/ingredients/ingr_id_graph.gexf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mingredients_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'ingredients_ids' is not defined"
     ]
    }
   ],
   "source": [
    "# ingr_id_graph = nx.Graph()\n",
    "path = '/media/antonio/WD1T/datasets allrecipes and epicurious/allrecipes/'\n",
    "#     path = 'data/allrecipes/'\n",
    "c=0\n",
    "ingredient_ids = set()\n",
    "for folder_number in sorted(map(int, os.listdir(path))):\n",
    "    folder_name = str(folder_number)\n",
    "    folder_path = path + folder_name\n",
    "    for file_name in sorted(map(lambda x: int(x[:-5]), os.listdir(folder_path))):\n",
    "        file_name = str(file_name) + '.html'\n",
    "        file_path = folder_path + '/' + file_name\n",
    "        with open(file_path) as f:\n",
    "            soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "#             recipe_info = soup.find('recipe-signup')\n",
    "#             recipe_id = recipe_info['data-id']\n",
    "#             recipe_title = recipe_info['data-title'][1:-1].encode('utf_8').decode('unicode_escape')\n",
    "#             info_row = [recipe_id, recipe_title]\n",
    "\n",
    "            ingredient_amounts = soup.select('section.recipe-ingredients span[itemprop=ingredients]')\n",
    "            for ingredient_amount in ingredient_amounts:\n",
    "                ingredient_id = ingredient_amount['data-id']\n",
    "                content = ingredient_amount.text.strip()\n",
    "                if content:\n",
    "                    tokens = nltk.word_tokenize(content)\n",
    "                    singularized = ' '.join(map(singularize, tokens))\n",
    "#                     found_ingr = False\n",
    "                    for ingredient in ingredients:\n",
    "                        if ingredient in singularized:\n",
    "#                             found_ingr = True\n",
    "                            ingredient_ids.add(ingredient_id)\n",
    "#                             if ingr_id_graph.has_edge(ingredient_id, ingredient):\n",
    "#                                 ingr_id_graph[ingredient_id][ingredient]['weight'] += 1\n",
    "#                             else:\n",
    "#                                 ingr_id_graph.add_edge(ingredient_id, ingredient, weight=1)\n",
    "#                     if not found_ingr:\n",
    "#                         writer3.writerow(['INGREDIENT', file_path, singularized])\n",
    "#             ingredient_row = info_row + list(ingredient_ids)\n",
    "#             writer1.writerow(ingredient_row)\n",
    "            c+=1\n",
    "            if c % 10000 == 0:\n",
    "                print(c, 'recipes processed.')\n",
    "# nx.write_gexf(ingr_id_graph, 'data/ingredients/ingr_id_graph.gexf')\n",
    "print(len(ingredients_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ingredient_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-fdfdadd9392e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mingredient_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'ingredient_ids' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(ingredient_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Store ingredient_ids on a pickle object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12242"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingr_id_graph.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43837"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingr_id_graph.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8263"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=0\n",
    "for n in ingr_id_graph.nodes(data=False):\n",
    "#     ns = ingr_id_graph.neighbors(n)\n",
    "#     for ne in ns:\n",
    "    anyy=False\n",
    "    for ne in ingr_id_graph[n]:\n",
    "        if ingr_id_graph[n][ne]['weight'] > 1:\n",
    "#             print(n)\n",
    "#             print(ingr_id_graph[n])\n",
    "#             print()\n",
    "            anyy=True\n",
    "            break\n",
    "    if anyy:\n",
    "        c+=1\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ingrs = set()\n",
    "for n in ingr_id_graph.nodes(data=False):\n",
    "#     ns = ingr_id_graph.neighbors(n)\n",
    "#     for ne in ns:\n",
    "    for ne in ingr_id_graph[n]:\n",
    "        if ingr_id_graph[n][ne]['weight'] > 1:\n",
    "#             print(n)\n",
    "#             print(ingr_id_graph[n])\n",
    "#             print()\n",
    "            anyy=True\n",
    "            break\n",
    "    if anyy:\n",
    "        c+=1\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qwe=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1382"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=0\n",
    "nums=set()\n",
    "for n in ingr_id_graph.nodes(data=False):\n",
    "#     ns = ingr_id_graph.neighbors(n)\n",
    "#     for ne in ns:\n",
    "    \n",
    "    if not n.isdigit():\n",
    "        c+=1\n",
    "        nums.add(n)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nums2=set()\n",
    "for n in ingr_id_graph.nodes(data=False):\n",
    "#     ns = ingr_id_graph.neighbors(n)\n",
    "#     for ne in ns:\n",
    "    \n",
    "    if n.isdigit():\n",
    "        ns = ingr_id_graph.neighbors(n)\n",
    "        for ne in ns:\n",
    "            if ne.isdigit():\n",
    "                print(n, ne)\n",
    "            nums2.add(ne)\n",
    "#         print(ingr_id_graph[n])\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1382"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nums2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['21046',\n",
       " '4620',\n",
       " '24098',\n",
       " '2159',\n",
       " '1606',\n",
       " '21771',\n",
       " '25719',\n",
       " '4617',\n",
       " '23688',\n",
       " '21572']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nums2)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1382"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nx.write_gexf(ingr_id_graph, 'data/ingredients/ingr_id_graph.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with open('data/allrecipes/6000/6903.html') as f:\n",
    "#     soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "#     info = soup.find('recipe-signup')\n",
    "#     print(info['data-id'])\n",
    "#     print(info['data-title'])\n",
    "#     qq=info['data-title']\n",
    "#     info_row = [info['data-id'], info['data-title']]\n",
    "#     technique_names = []\n",
    "#     instructions = soup.select('section.recipe-directions span.recipe-directions__list--item')\n",
    "#     for instruction in instructions:\n",
    "#         infinitived = ' '.join(map(infinitive, instruction.string.strip().split()))\n",
    "#         for technique in techniques:\n",
    "#             if technique in infinitived:\n",
    "#                 technique_names.append(technique)\n",
    "#     technique_row = info_row + technique_names\n",
    "#     print(technique_row)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# las tecnicas no tienen pq ser solo verbos - pueden ser sustantivos, por ejemplo, bbq, fondue..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for folder_name in os.listdir('data/allrecipes/'):\n",
    "#     folder_path = 'data/allrecipes/' + folder_name\n",
    "#     for file_name in os.listdir(folder_path):\n",
    "#         file_path = folder_path + '/' + file_name\n",
    "#         with open(file_path) as f:\n",
    "#             soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "#             info = soup.find('recipe-signup')\n",
    "#             print(info['data-id'])\n",
    "#             print(info['data-title'])\n",
    "#             ingredients = soup.select('span[itemprop=ingredients]')\n",
    "#             for ingredient in ingredients:\n",
    "#                 print(ingredient.string.strip())\n",
    "#             print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
