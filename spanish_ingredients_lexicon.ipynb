{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "from itertools import permutations\n",
    "\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import requests\n",
    "from nltk.corpus import cess_esp\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.util import ngrams\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "# client.drop_database('lexicon')\n",
    "db = client.lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with open('data/es_lexicon.csv') as f:\n",
    "#     reader = csv.reader(\n",
    "#         f,\n",
    "#         delimiter=' ',\n",
    "#     )\n",
    "#     docs = []\n",
    "#     count = 0\n",
    "#     for row in reader:\n",
    "#         for i in range(1, len(row[1:]), 2):\n",
    "#             entry = {}\n",
    "#             entry['flexion'] = row[0].lower()\n",
    "#             entry['lemma'] = row[i].lower()\n",
    "#             entry['eagle'] = row[i+1].lower()\n",
    "#             docs.append(entry)\n",
    "#             count += 1\n",
    "#         if count % 1000 == 0:\n",
    "#             db.es_lexicon.insert_many(docs)\n",
    "#             docs = []\n",
    "#     db.es_lexicon.insert_many(docs)\n",
    "#     docs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "668825"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.es_lexicon.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spanish_stopwords = set()\n",
    "with open('data/spanish_stopwords.txt') as f:\n",
    "    for line in f:\n",
    "        word = line.strip()\n",
    "        spanish_stopwords.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tagged_sp_sents = cess_esp.tagged_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# size = int(len(tagged_sp_sents) * 0.1)\n",
    "# train_sp_sents = tagged_sp_sents[size:]\n",
    "# test_sp_sents = tagged_sp_sents[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tagged_sp_words = cess_esp.tagged_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tags = [tag for (word, tag) in tagged_sp_words]\n",
    "# most_freq_tags = nltk.FreqDist(tags)\n",
    "# most_freq_tags.most_common()[:10]\n",
    "\n",
    "# [('sps00', 25272),\n",
    "#  ('ncms000', 11428),\n",
    "#  ('Fc', 11420),\n",
    "#  ('ncfs000', 11008),\n",
    "#  ('da0fs0', 6838),\n",
    "#  ('da0ms0', 6012),\n",
    "#  ('rg', 5937),\n",
    "#  ('Fp', 5866),\n",
    "#  ('cc', 5854),\n",
    "#  ('ncmp000', 5711)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# default_tag = 'ncms000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# t0 = nltk.DefaultTagger(None)\n",
    "# t1 = nltk.UnigramTagger(train_sp_sents, backoff=t0)\n",
    "# t2 = nltk.BigramTagger(train_sp_sents, backoff=t1)\n",
    "# sp_tagger = nltk.TrigramTagger(train_sp_sents, backoff=t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sp_tagger.evaluate(test_sp_sents)\n",
    "\n",
    "# 0.8808231173380034"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('data/sp_tagger.pickle', 'wb') as f:\n",
    "#     pickle.dump(sp_tagger, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data/sp_tagger.pickle', 'rb') as f:\n",
    "    sp_tagger = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_spanish_ingredients_file(filename):\n",
    "    return filename.startswith('es_') and filename.endswith('_ingredients.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_node(g, n):\n",
    "    if not n in g:\n",
    "        g.add_node(n, count=0, represent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_edge(g, n1, n2):\n",
    "    if n1 != n2 and not nx.has_path(g, n1, n2):\n",
    "        g.add_edge(n1, n2)\n",
    "        ns = nx.node_connected_component(g, n1)\n",
    "        rs = [n for n in ns if g.node[n]['represent']]\n",
    "        for r1 in rs:\n",
    "            for r2 in rs:\n",
    "                if r1 != r2 and sublist(nltk.word_tokenize(r2), nltk.word_tokenize(r1)):\n",
    "                    g.node[r1]['represent'] = False\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sublist(a, b):\n",
    "    res = False\n",
    "    for i in range(len(b)-len(a)+1):\n",
    "        if b[i:i+len(a)] == a:\n",
    "            res = True\n",
    "            break\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_syn = nx.Graph()\n",
    "ingredients_root = 'data/ingredients/'\n",
    "for e in os.listdir(ingredients_root):\n",
    "    file_path = ingredients_root + e\n",
    "    if os.path.isfile(file_path):\n",
    "        if is_spanish_ingredients_file(e):\n",
    "            with open(file_path) as f:\n",
    "                for line in f:\n",
    "                    syn_set = set()\n",
    "                    ingrs1 = line.strip()\n",
    "                    for ingrs2 in ingrs1.split(' / '):\n",
    "                        for ingrs3 in ingrs2.split(' o '):\n",
    "                            for ingr in ingrs3.split(' - '):\n",
    "                                syn_set.add(ingr)\n",
    "                                add_node(graph_syn, ingr)\n",
    "                    syn_set = list(syn_set)\n",
    "                    i1 = syn_set[0]\n",
    "                    for i2 in syn_set[1:]:\n",
    "                        add_edge(graph_syn, i1, i2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3327"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3319"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([n for n, d in graph_syn.nodes_iter(data=True) if d['represent']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3222"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.number_connected_components(graph_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nx.write_gexf(graph_syn, 'data/spanish_ingredients_lexicon_1.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_syn = nx.read_gexf('data/spanish_ingredients_lexicon_1.gexf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_mapping = {\n",
    "    'a': 'adj',\n",
    "    'r': 'adv',\n",
    "    'd': 'det',\n",
    "    'n': 'noun',\n",
    "    'v': 'verb',\n",
    "    'p': 'pron',\n",
    "    'c': 'conj',\n",
    "    'i': 'interj',\n",
    "    's': 'prep',\n",
    "    'f': 'punt',\n",
    "    'z': 'num',\n",
    "    'w': 'date-time',\n",
    "}\n",
    "\n",
    "def map_tag(eagle):\n",
    "    return tag_mapping[eagle[0]]\n",
    "\n",
    "def get_category(entry):\n",
    "    if entry['eagle'][0] == 'v' and entry['eagle'][2] == 'p':\n",
    "        categ = 'adj'\n",
    "    else:\n",
    "        categ = map_tag(entry['eagle'])\n",
    "    return categ\n",
    "\n",
    "def has_category(category, entries):\n",
    "    return category in map(get_category, entries)\n",
    "\n",
    "def is_number(x):\n",
    "    return x in ['dos', 'tres', 'cuatro', 'cinco', 'seis', 'siete', 'ocho', 'nueve']\n",
    "\n",
    "def ingredient_tagger(x):\n",
    "    result = []\n",
    "    tokens = nltk.word_tokenize(x)\n",
    "    if len(tokens) == 1:\n",
    "        result.append((x, 'noun'))\n",
    "    else:\n",
    "        tags = sp_tagger.tag(tokens)\n",
    "        for token, tag in tags:\n",
    "            if is_number(token):\n",
    "                tag = 'num'\n",
    "            elif tag:\n",
    "                tag = map_tag(tag.lower())\n",
    "                if tag == 'verb':\n",
    "                    res = list(db.es_lexicon.find({'flexion': token}))\n",
    "                    if res:\n",
    "                        if has_category('adj', res):\n",
    "                            tag = 'adj'\n",
    "                        elif has_category('noun', res):\n",
    "                            tag = 'noun'\n",
    "            else:\n",
    "                res = list(db.es_lexicon.find({'flexion': token}))\n",
    "                if res:\n",
    "                    if has_category('adj', res):\n",
    "                        tag = 'adj'\n",
    "                    elif has_category('noun', res):\n",
    "                        tag = 'noun'\n",
    "                    elif has_category('verb', res):\n",
    "                        tag = 'verb'\n",
    "                    elif has_category('det', res):\n",
    "                        tag = 'det'\n",
    "                    elif has_category('pron', res):\n",
    "                        tag = 'pron'\n",
    "                    elif has_category('prep', res):\n",
    "                        tag = 'prep'\n",
    "                    elif has_category('num', res):\n",
    "                        tag = 'num'\n",
    "                    else:\n",
    "                        tag = get_category(res[0])\n",
    "                else:\n",
    "                    tag = 'noun'\n",
    "            result.append((token, tag))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# with open('data/spanish_ingredients_postags.csv', 'w') as f:\n",
    "#     writer = csv.writer(\n",
    "#         f,\n",
    "#         delimiter=',',\n",
    "#         quotechar='\"',\n",
    "#         quoting=csv.QUOTE_MINIMAL\n",
    "#     )\n",
    "#     for ingr, dat in graph_syn.nodes_iter(data=True):\n",
    "#         if dat['representative']:\n",
    "#             pos_tag = ' '.join(tag for token, tag in ingredient_tagger(ingr))\n",
    "#             row = [ingr, pos_tag]\n",
    "#             writer.writerow(row)\n",
    "\n",
    "# CPU times: user 6.56 s, sys: 356 ms, total: 6.91 s\n",
    "# Wall time: 22min 14s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "postags = {}\n",
    "with open('data/spanish_ingredients_postags.csv') as f:\n",
    "    reader = csv.reader(\n",
    "        f,\n",
    "        delimiter=',',\n",
    "    )\n",
    "    for row in reader:\n",
    "        postags[row[0]] = row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pimientos', 'noun'), ('verdes', 'adj')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_postags(x):\n",
    "    try:\n",
    "        tags = postags[x]\n",
    "    except:\n",
    "        postags[x] = ' '.join(tag for token, tag in ingredient_tagger(x))\n",
    "        tags = postags[x]\n",
    "    return list(zip(nltk.word_tokenize(x),nltk.word_tokenize(tags)))\n",
    "\n",
    "# Example\n",
    "get_postags('pimientos verdes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# apicultur synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nouns = set()\n",
    "for ingr in graph_syn.nodes_iter():\n",
    "    for token, tag in get_postags(ingr):\n",
    "        if tag == 'noun':\n",
    "            nouns.add(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1620"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with open('data/apicultur_ingredients_synonyms.csv', 'w') as f:\n",
    "#     writer = csv.writer(\n",
    "#         f,\n",
    "#         delimiter=',',\n",
    "#         quotechar='\"',\n",
    "#         quoting=csv.QUOTE_MINIMAL\n",
    "#     )\n",
    "#     base_url = 'https://store.apicultur.com/api/sinonimosporpalabra/1.0.0/'\n",
    "#     headers = {'Authorization': 'Bearer uHS_7Q2Esg7XsUKNsaqFx2sB1mca'}\n",
    "#     count = 0\n",
    "#     for noun in nouns:\n",
    "#         if noun in graph_syn:\n",
    "#             url = base_url + noun\n",
    "#             response = requests.get(url, headers=headers)\n",
    "#             if response.text:\n",
    "#                 js = response.json()\n",
    "#                 row = [noun]\n",
    "#                 for d in js:\n",
    "#                     row.append(d['valor'])\n",
    "#                 writer.writerow(row)\n",
    "#             time.sleep(1)\n",
    "#         count += 1\n",
    "#         if count % 50 == 0:\n",
    "#             time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "apicultur_syns = {}\n",
    "with open('data/apicultur_ingredients_synonyms.csv') as f:\n",
    "    reader = csv.reader(\n",
    "        f,\n",
    "        delimiter=',',\n",
    "    )\n",
    "    for row in reader:\n",
    "        apicultur_syns[row[0]] = row[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apicultur_graph = nx.Graph()\n",
    "for k in apicultur_syns:\n",
    "    syns = apicultur_syns[k]\n",
    "    for syn in syns:\n",
    "        apicultur_graph.add_edge(k, syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.number_connected_components(apicultur_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_kn_complete(g):\n",
    "    complete = True\n",
    "    for n1 in g:\n",
    "        for n2 in g:\n",
    "            if n1 != n2 and not g.has_edge(n1, n2):\n",
    "                complete = False\n",
    "                break\n",
    "        if not complete:\n",
    "            break\n",
    "    return complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kn_complete_graphs = []\n",
    "for subg in nx.connected_component_subgraphs(apicultur_graph):\n",
    "    if is_kn_complete(subg):\n",
    "        kn_complete_graphs.append(subg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kn_complete_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syns_found = 0\n",
    "for g in kn_complete_graphs:\n",
    "    syn_set = g.nodes()\n",
    "    i1 = syn_set[0]\n",
    "    add_node(graph_syn, i1)\n",
    "    for i2 in syn_set[1:]:\n",
    "        add_node(graph_syn, i2)\n",
    "        add_edge(graph_syn, i1, i2)\n",
    "        syns_found += 1\n",
    "syns_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3354"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3346"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([n for n, d in graph_syn.nodes_iter(data=True) if d['represent']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_syn.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3210"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.number_connected_components(graph_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nx.write_gexf(graph_syn, 'data/spanish_ingredients_lexicon_2.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_syn = nx.read_gexf('data/spanish_ingredients_lexicon_2.gexf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Singular and plural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pimientos verdes'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_singular_word(word_tag):\n",
    "    word = word_tag[0]\n",
    "    tag = word_tag[1]\n",
    "    return tag in ['adj', 'noun'] and not word.endswith('s')\n",
    "\n",
    "def is_singular_ingredient(ingredient):\n",
    "    return any(map(is_singular_word, get_postags(ingredient)))\n",
    "\n",
    "def naive_pluralize(word):\n",
    "    if word[-1] in 'aeiou':\n",
    "        plural = word + 's'\n",
    "    elif word[-1] == 'z':\n",
    "        plural = word[:-1] + 'ces'\n",
    "    else:\n",
    "        plural = word + 'es'\n",
    "    return plural\n",
    "\n",
    "def pluralize_adj(word):\n",
    "    if not set('áéíóú').intersection(word):\n",
    "        return naive_pluralize(word)\n",
    "    \n",
    "    singular = word\n",
    "    r1 = db.es_lexicon.find_one({'flexion': word, 'eagle': {'$regex': 'a...s.*'}})\n",
    "    if r1:\n",
    "        lemma = r1['lemma']\n",
    "        eagle = r1['eagle'][:4] + 'p' + r1['eagle'][5:]\n",
    "        r2 = db.es_lexicon.find_one({'lemma': lemma, 'eagle': eagle})\n",
    "        if r2:\n",
    "            singular = r2['flexion']\n",
    "    return singular\n",
    "\n",
    "def pluralize_noun(word):\n",
    "    if not set('áéíóú').intersection(word):\n",
    "        return naive_pluralize(word)\n",
    "    \n",
    "    singular = word\n",
    "    r1 = db.es_lexicon.find_one({'flexion': word, 'eagle': {'$regex': 'n..s.*'}})\n",
    "    if r1:\n",
    "        lemma = r1['lemma']\n",
    "        eagle = r1['eagle'][:3] + 'p' + r1['eagle'][4:]\n",
    "        r2 = db.es_lexicon.find_one({'lemma': lemma, 'eagle': eagle})\n",
    "        if r2:\n",
    "            singular = r2['flexion']\n",
    "    return singular\n",
    "\n",
    "def pluralize_verb(word):\n",
    "    singular = word\n",
    "    r1 = db.es_lexicon.find_one({'flexion': word, 'eagle': {'$regex': 'v....s.*'}})\n",
    "    if r1:\n",
    "        lemma = r1['lemma']\n",
    "        eagle = r1['eagle'][:5] + 'p' + r1['eagle'][6:]\n",
    "        r2 = db.es_lexicon.find_one({'lemma': lemma, 'eagle': eagle})\n",
    "        if r2:\n",
    "            singular = r2['flexion']\n",
    "    return singular\n",
    "\n",
    "def pluralize_det(word):\n",
    "    singular = word\n",
    "    r1 = db.es_lexicon.find_one({'flexion': word, 'eagle': {'$regex': 'd...s.*'}})\n",
    "    if r1:\n",
    "        lemma = r1['lemma']\n",
    "        eagle = r1['eagle'][:4] + 'p' + r1['eagle'][5:]\n",
    "        r2 = db.es_lexicon.find_one({'lemma': lemma, 'eagle': eagle})\n",
    "        if r2:\n",
    "            singular = r2['flexion']\n",
    "    return singular\n",
    "\n",
    "def pluralize_word(word_tag):\n",
    "    word = word_tag[0]\n",
    "    tag = word_tag[1]\n",
    "    plural = word\n",
    "    if word.isalpha():\n",
    "        if tag == 'adj':\n",
    "            plural = pluralize_adj(word)\n",
    "        elif tag == 'noun':\n",
    "            plural = pluralize_noun(word)\n",
    "        elif tag == 'verb':\n",
    "            singular = pluralize_verb(word)\n",
    "        elif tag == 'det':\n",
    "            singular = pluralize_det(word)\n",
    "    return plural\n",
    "\n",
    "def pluralize_ingredient(ingredient):\n",
    "    plurals = map(pluralize_word, get_postags(ingredient))\n",
    "    return ' '.join(plurals)\n",
    "\n",
    "# Example\n",
    "pluralize_ingredient('pimiento verde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pimiento verde'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_plural_word(word_tag):\n",
    "    word = word_tag[0]\n",
    "    tag = word_tag[1]\n",
    "    return tag in ['adj', 'noun'] and word.endswith('s')\n",
    "\n",
    "def is_plural_ingredient(ingredient):\n",
    "    return any(map(is_plural_word, get_postags(ingredient)))\n",
    "\n",
    "def naive_singularize(word):\n",
    "    return word[:-1]\n",
    "\n",
    "def singularize_adj(word):\n",
    "    if not set('áéíóú').intersection(word) and len(word) > 1 and word[-2] in 'aiou':\n",
    "        return naive_singularize(word)\n",
    "    \n",
    "    singular = word\n",
    "    r1 = db.es_lexicon.find_one({'flexion': word, 'eagle': {'$regex': 'a...p.*'}})\n",
    "    if r1:\n",
    "        lemma = r1['lemma']\n",
    "        eagle = r1['eagle'][:4] + 's' + r1['eagle'][5:]\n",
    "        r2 = db.es_lexicon.find_one({'lemma': lemma, 'eagle': eagle})\n",
    "        if r2:\n",
    "            singular = r2['flexion']\n",
    "    return singular\n",
    "\n",
    "def singularize_noun(word):\n",
    "    if not set('áéíóú').intersection(word) and len(word) > 1 and word[-2] in 'aiou':\n",
    "        return naive_singularize(word)\n",
    "    \n",
    "    singular = word\n",
    "    r1 = db.es_lexicon.find_one({'flexion': word, 'eagle': {'$regex': 'n..p.*'}})\n",
    "    if r1:\n",
    "        lemma = r1['lemma']\n",
    "        eagle = r1['eagle'][:3] + 's' + r1['eagle'][4:]\n",
    "        r2 = db.es_lexicon.find_one({'lemma': lemma, 'eagle': eagle})\n",
    "        if r2:\n",
    "            singular = r2['flexion']\n",
    "    return singular\n",
    "\n",
    "def singularize_verb(word):\n",
    "    singular = word\n",
    "    r1 = db.es_lexicon.find_one({'flexion': word, 'eagle': {'$regex': 'v....p.*'}})\n",
    "    if r1:\n",
    "        lemma = r1['lemma']\n",
    "        eagle = r1['eagle'][:5] + 's' + r1['eagle'][6:]\n",
    "        r2 = db.es_lexicon.find_one({'lemma': lemma, 'eagle': eagle})\n",
    "        if r2:\n",
    "            singular = r2['flexion']\n",
    "    return singular\n",
    "\n",
    "def singularize_det(word):\n",
    "    singular = word\n",
    "    r1 = db.es_lexicon.find_one({'flexion': word, 'eagle': {'$regex': 'd...p.*'}})\n",
    "    if r1:\n",
    "        lemma = r1['lemma']\n",
    "        eagle = r1['eagle'][:4] + 's' + r1['eagle'][5:]\n",
    "        r2 = db.es_lexicon.find_one({'lemma': lemma, 'eagle': eagle})\n",
    "        if r2:\n",
    "            singular = r2['flexion']\n",
    "    return singular\n",
    "\n",
    "def singularize_word(word_tag):\n",
    "    word = word_tag[0]\n",
    "    tag = word_tag[1]\n",
    "    singular = word\n",
    "    if word.isalpha():\n",
    "        if tag == 'adj':\n",
    "            singular = singularize_adj(word)\n",
    "        elif tag == 'noun':\n",
    "            singular = singularize_noun(word)\n",
    "        elif tag == 'verb':\n",
    "            singular = singularize_verb(word)\n",
    "        elif tag == 'det':\n",
    "            singular = singularize_det(word)\n",
    "    return singular\n",
    "\n",
    "def singularize_ingredient(ingredient):\n",
    "    singulars = map(singularize_word, get_postags(ingredient))\n",
    "    return ' '.join(singulars)\n",
    "\n",
    "# Example\n",
    "singularize_ingredient('pimientos verdes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# for ingr, dat in graph_syn.nodes(data=True):\n",
    "#     if dat['represent']:\n",
    "#         if is_singular_ingredient(ingr):\n",
    "#             plural = pluralize_ingredient(ingr)\n",
    "#             add_node(graph_syn, plural)\n",
    "#             add_edge(graph_syn, ingr, plural)\n",
    "#         if is_plural_ingredient(ingr):\n",
    "#             singular = singularize_ingredient(ingr)\n",
    "#             add_node(graph_syn, singular)\n",
    "#             add_edge(graph_syn, ingr, singular)\n",
    "\n",
    "# CPU times: user 8.66 s, sys: 120 ms, total: 8.78 s\n",
    "# Wall time: 8min 5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# len(graph_syn)\n",
    "\n",
    "# 6807"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# len([n for n, d in graph_syn.nodes_iter(data=True) if d['represent']])\n",
    "\n",
    "# 6796"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# graph_syn.number_of_edges()\n",
    "\n",
    "# 3695"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3112"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nx.number_connected_components(graph_syn)\n",
    "\n",
    "# 3112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nx.write_gexf(graph_syn, 'data/spanish_ingredients_lexicon_3.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_syn = nx.read_gexf('data/spanish_ingredients_lexicon_3.gexf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Numbers\n",
    "def numbers(x):\n",
    "    return x.replace(' 1 ', ' uno ') \\\n",
    "            .replace(' 2 ', ' dos ') \\\n",
    "            .replace(' 3 ', ' tres ') \\\n",
    "            .replace(' 4 ', ' cuatro ') \\\n",
    "            .replace(' 5 ', ' cinco ') \\\n",
    "            .replace(' 6 ', ' seis ') \\\n",
    "            .replace(' 7 ', ' siete ') \\\n",
    "            .replace(' 8 ', ' ocho ') \\\n",
    "            .replace(' 9 ', ' nueve ')\n",
    "\n",
    "# Accent marks on vowels - {'á', 'ã', 'ç', 'è', 'é', 'ê', 'í', 'ñ', 'ò', 'ó', 'ú', 'ü', 'ō'}\n",
    "def accent_marks(x):\n",
    "    return x.replace('á', 'a') \\\n",
    "            .replace('ã', 'a') \\\n",
    "            .replace('è', 'e') \\\n",
    "            .replace('é', 'e') \\\n",
    "            .replace('ê', 'e') \\\n",
    "            .replace('í', 'i') \\\n",
    "            .replace('ò', 'o') \\\n",
    "            .replace('ó', 'o') \\\n",
    "            .replace('ō', 'o') \\\n",
    "            .replace('ú', 'u') \\\n",
    "            .replace('ü', 'u')\n",
    "\n",
    "# Non-ascii consonants - {'á', 'ã', 'ç', 'è', 'é', 'ê', 'í', 'ñ', 'ò', 'ó', 'ú', 'ü', 'ō'}\n",
    "def nonascii_consonants(x):\n",
    "    return x.replace('ç', 'c') \\\n",
    "            .replace('ñ', 'n')\n",
    "    \n",
    "# Dashes (-)\n",
    "def dashes1(x):\n",
    "    return x.replace('-', ' ')\n",
    "\n",
    "def dashes2(x):\n",
    "    return x.replace('-', '')\n",
    "\n",
    "# POS tags\n",
    "# ADJETIVOS .... A ADJ ...... X\n",
    "# ADVERBIOS .... R ADV\n",
    "# DETERMINANTES  D DET\n",
    "# NOMBRES ...... N NOUN ..... X\n",
    "# VERBOS ....... V VERB ..... X\n",
    "# PRONOMBRES ... P PRON\n",
    "# CONJUNCIONES . C CONJ\n",
    "# INTERJECCIONES I INTERJ\n",
    "# PREPOSICIONES  S PREP\n",
    "# PUNTUACIÓN ... F PUNTUATION\n",
    "# NUMERALES .... Z NUM ...... X\n",
    "# FECHAS Y HORAS W DATE-TIME\n",
    "def pos_tags(x):\n",
    "    tags = get_postags(x)\n",
    "    filtered = [token\n",
    "                for token, tag in tags\n",
    "                if tag in ['adj', 'noun', 'verb', 'num']\n",
    "               ]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "def itself(x):\n",
    "    return x\n",
    "\n",
    "funcs = [itself, pos_tags, numbers, accent_marks, nonascii_consonants, dashes1, dashes2]\n",
    "combinations = []\n",
    "for i in range(1, len(funcs) + 1):\n",
    "    combinations.append(list(itertools.combinations(funcs, i)))\n",
    "combinations = [c for comb in combinations for c in comb]\n",
    "\n",
    "# def normalize(ingredient): # original time consuming version\n",
    "#     result = set()\n",
    "#     for c in combinations:\n",
    "#         x = ingredient\n",
    "#         for f in c:\n",
    "#             x = f(x)\n",
    "#         result.add(x)\n",
    "#     return result\n",
    "\n",
    "def normalize(ingredient): # dynamic programming version\n",
    "    result = set()\n",
    "    for c in combinations:\n",
    "        x = ingredient\n",
    "        for f in c:\n",
    "            if not x in d[f.__name__]:\n",
    "                d[f.__name__][x] = f(x)\n",
    "            x = d[f.__name__][x]\n",
    "        result.add(x)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([list(map(lambda x: x.__name__, c)) for c in combinations])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# d = defaultdict(dict)\n",
    "\n",
    "# or\n",
    "\n",
    "with open('data/spanish_ingredients_normalization.pickle', 'rb') as f:\n",
    "    d = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# c=0\n",
    "# for ingr, dat in graph_syn.nodes(data=True):\n",
    "#     if len(nltk.word_tokenize(ingr)) < 4:\n",
    "#         if dat['represent']:\n",
    "#             norms = normalize(ingr)\n",
    "#             for norm in norms:\n",
    "#                 add_node(graph_syn, norm)\n",
    "#                 add_edge(graph_syn, ingr, norm)\n",
    "#     c+=1\n",
    "#     if c%100==0:\n",
    "#         print(c)\n",
    "\n",
    "# d = dict(d)\n",
    "\n",
    "# CPU times: user 12.4 s, sys: 324 ms, total: 12.8 s\n",
    "# Wall time: 18min 55s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data/spanish_ingredients_normalization.pickle', 'wb') as f:\n",
    "    pickle.dump(d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10899"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(graph_syn)\n",
    "\n",
    "# 10899"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10867"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len([n for n, d in graph_syn.nodes_iter(data=True) if d['represent']])\n",
    "\n",
    "# 10867"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7813"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# graph_syn.number_of_edges()\n",
    "\n",
    "# 7813"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3086"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nx.number_connected_components(graph_syn)\n",
    "\n",
    "# 3086"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nx.write_gexf(graph_syn, 'data/spanish_ingredients_lexicon_4.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_syn = nx.read_gexf('data/spanish_ingredients_lexicon_4.gexf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def my_ngrams(ingredient):\n",
    "    ngrms = []\n",
    "    tokens = nltk.word_tokenize(ingredient)\n",
    "    for i in range(1, len(tokens) + 1):\n",
    "        ngrms.extend(ngrams(tokens, i))\n",
    "    return list(map(lambda x: ' '.join(x), ngrms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lengths = defaultdict(int)\n",
    "for ingr in graph_syn.nodes_iter():\n",
    "    lengths[len(nltk.word_tokenize(ingr))] += 1\n",
    "lengths = dict(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 2103,\n",
       " 2: 4502,\n",
       " 3: 4467,\n",
       " 4: 1210,\n",
       " 5: 578,\n",
       " 6: 205,\n",
       " 7: 51,\n",
       " 8: 25,\n",
       " 9: 23,\n",
       " 10: 8}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# representative_syns_dict = {}\n",
    "\n",
    "# or\n",
    "\n",
    "with open('data/spanish_representative_ingredients.pickle', 'rb') as f:\n",
    "    representative_syns_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'salsa de tomate', 'salsa tomate', 'salsas de tomates', 'salsas tomates'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def representative_syns(ingredient):\n",
    "    try:\n",
    "        result = representative_syns_dict[ingredient]\n",
    "    except:\n",
    "        result = set()\n",
    "        syns = nx.node_connected_component(graph_syn, ingredient)\n",
    "        for syn in syns:\n",
    "            dat = graph_syn.node[syn]\n",
    "            if dat['represent']:\n",
    "                result.add(syn)\n",
    "        representative_syns_dict[ingredient] = result\n",
    "    return result\n",
    "\n",
    "representative_syns('salsa de tomate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['salsa de tomate'],\n",
       " ['salsa', 'de tomate'],\n",
       " ['salsa de', 'tomate'],\n",
       " ['salsa', 'de', 'tomate']]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ngram_combinations(ingredient):\n",
    "    combs = []\n",
    "    ngram_list = my_ngrams(ingredient)\n",
    "    for i in range(1, len(ngram_list) + 1):\n",
    "        combs.extend(permutations(ngram_list, i))\n",
    "    combs = [list(c) for c in combs if ' '.join(c) == ingredient]\n",
    "    return combs\n",
    "\n",
    "# Example\n",
    "ngram_combinations('salsa de tomate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['salsa de tomates',\n",
       " 'salsa de tomate',\n",
       " 'salsas de tomate',\n",
       " 'salsas de tomates',\n",
       " 'salsas tomatil',\n",
       " 'salsa tomatil']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def comb_syns(expr, syn_dict):\n",
    "    res = set()\n",
    "    combs = ngram_combinations(expr)\n",
    "    for ngrms in combs:\n",
    "        syn_list = [syn_dict[ngrm] for ngrm in ngrms]\n",
    "        syn_comb = list(product(*syn_list))\n",
    "        for sc in syn_comb:\n",
    "            res.add(' '.join(sc))\n",
    "    return list(res)\n",
    "\n",
    "# Example\n",
    "expr= 'salsa de tomate'\n",
    "syn_dict = {\n",
    "    'salsa': ['salsa', 'salsas'],\n",
    "    'de': ['de'],\n",
    "    'tomate': ['tomate', 'tomates'],\n",
    "    'salsa de': ['salsa de'],\n",
    "    'de tomate': ['de tomate', 'tomatil'],\n",
    "    'salsa de tomate': ['salsa de tomate'],\n",
    "}\n",
    "comb_syns(expr, syn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ingr='puré de erizo'\n",
    "ngrms = my_ngrams(ingr)\n",
    "syn_dict = create_syn_dict(ngrms)\n",
    "for ngrm in ngrms:\n",
    "    if ngrm in graph_syn and ngrm not in syns1:\n",
    "        syns2 = representative_syns(ngrm)\n",
    "        syn_dict[ngrm] = syn_dict[ngrm].union(syns2)\n",
    "syn_combs = comb_syns(ingr, syn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'de': {'de'},\n",
       " 'de erizo': {'de erizo'},\n",
       " 'erizo': {'cerdo espin',\n",
       "  'cerdo espines',\n",
       "  'cerdo espín',\n",
       "  'cerdos espin',\n",
       "  'cerdos espines',\n",
       "  'cerdos espín',\n",
       "  'chancho espin',\n",
       "  'chancho espines',\n",
       "  'chancho espín',\n",
       "  'chanchos espin',\n",
       "  'chanchos espines',\n",
       "  'chanchos espín',\n",
       "  'erizo',\n",
       "  'erizos',\n",
       "  'puerco espin',\n",
       "  'puerco espines',\n",
       "  'puerco espín',\n",
       "  'puercos espin',\n",
       "  'puercos espines',\n",
       "  'puercos espín'},\n",
       " 'puré': {'pure', 'pures', 'puré', 'purés'},\n",
       " 'puré de': {'puré de'},\n",
       " 'puré de erizo': {'puré de erizo'}}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['purés de puerco espín',\n",
       " 'pures de cerdos espín',\n",
       " 'purés de puercos espin',\n",
       " 'puré de cerdo espin',\n",
       " 'purés de cerdo espín',\n",
       " 'pures de cerdo espín',\n",
       " 'purés de cerdo espines',\n",
       " 'pure de chancho espines',\n",
       " 'pures de chanchos espín',\n",
       " 'puré de erizos',\n",
       " 'puré de puerco espines',\n",
       " 'pure de erizos',\n",
       " 'pures de cerdo espin',\n",
       " 'purés de puerco espines',\n",
       " 'purés de puerco espin',\n",
       " 'purés de chanchos espín',\n",
       " 'purés de cerdo espin',\n",
       " 'pure de chanchos espin',\n",
       " 'puré de puercos espin',\n",
       " 'purés de erizo',\n",
       " 'puré de puerco espín',\n",
       " 'pure de chancho espín',\n",
       " 'purés de erizos',\n",
       " 'pure de puercos espín',\n",
       " 'pures de cerdos espines',\n",
       " 'pure de cerdos espín',\n",
       " 'puré de chanchos espin',\n",
       " 'pure de erizo',\n",
       " 'puré de cerdo espines',\n",
       " 'pure de chancho espin',\n",
       " 'pure de puerco espín',\n",
       " 'pures de puercos espín',\n",
       " 'pures de chanchos espin',\n",
       " 'pures de puercos espin',\n",
       " 'puré de chancho espines',\n",
       " 'purés de puercos espín',\n",
       " 'purés de chanchos espin',\n",
       " 'pure de puerco espin',\n",
       " 'pures de puercos espines',\n",
       " 'puré de cerdos espin',\n",
       " 'puré de puerco espin',\n",
       " 'pures de puerco espín',\n",
       " 'pures de chancho espines',\n",
       " 'pures de chancho espin',\n",
       " 'purés de chanchos espines',\n",
       " 'purés de chancho espin',\n",
       " 'pures de cerdo espines',\n",
       " 'puré de cerdos espín',\n",
       " 'pure de cerdos espin',\n",
       " 'pure de cerdo espin',\n",
       " 'purés de cerdos espín',\n",
       " 'pures de chancho espín',\n",
       " 'pures de chanchos espines',\n",
       " 'puré de chanchos espín',\n",
       " 'pures de erizos',\n",
       " 'pures de cerdos espin',\n",
       " 'puré de chancho espin',\n",
       " 'purés de chancho espín',\n",
       " 'purés de puercos espines',\n",
       " 'puré de chanchos espines',\n",
       " 'pure de cerdos espines',\n",
       " 'pure de puercos espin',\n",
       " 'pures de puerco espin',\n",
       " 'pure de puerco espines',\n",
       " 'puré de cerdos espines',\n",
       " 'puré de cerdo espín',\n",
       " 'purés de cerdos espines',\n",
       " 'pure de cerdo espines',\n",
       " 'pures de puerco espines',\n",
       " 'puré de puercos espín',\n",
       " 'purés de chancho espines',\n",
       " 'puré de chancho espín',\n",
       " 'pure de cerdo espín',\n",
       " 'puré de erizo',\n",
       " 'pure de chanchos espín',\n",
       " 'pure de chanchos espines',\n",
       " 'purés de cerdos espin',\n",
       " 'pure de puercos espines',\n",
       " 'puré de puercos espines',\n",
       " 'pures de erizo']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn_combs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'de': {'de'},\n",
       " 'de tomate': {'de tomate'},\n",
       " 'salsa': {'salsa'},\n",
       " 'salsa de': {'salsa de'},\n",
       " 'salsa de tomate': {'salsa de tomate'},\n",
       " 'tomate': {'tomate'}}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_syn_dict(ngrms):\n",
    "    d = {}\n",
    "    for ngrm in ngrms:\n",
    "        d[ngrm] = set([ngrm])\n",
    "    return d\n",
    "\n",
    "# Example\n",
    "create_syn_dict(my_ngrams('salsa de tomate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for ingr in ['infusión piña verde pino']:\n",
    "#     if 1 < len(nltk.word_tokenize(ingr)) < 4:\n",
    "#         if graph_syn.node[ingr]['represent']:\n",
    "#             ngrms = my_ngrams(ingr)\n",
    "#             syn_dict = create_syn_dict(ngrms)\n",
    "#             for ngrm in ngrms:\n",
    "#                 if ngrm in graph_syn and ngrm not in syns1:\n",
    "#                     syns2 = representative_syns(ngrm)\n",
    "#                     syn_dict[ngrm] = syn_dict[ngrm].union(syns2)\n",
    "#             syn_combs = comb_syns(ingr, syn_dict)\n",
    "#             for syn_ingr in syn_combs:\n",
    "#                 add_node(graph_syn, syn_ingr)\n",
    "#                 add_edge(graph_syn, ingr, syn_ingr)\n",
    "#                 print('Adding edge', ingr, syn_ingr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "CPU times: user 28min 20s, sys: 292 ms, total: 28min 20s\n",
      "Wall time: 28min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "c=0\n",
    "\n",
    "ccs = list(nx.connected_components(graph_syn))\n",
    "for syns1 in ccs:\n",
    "    for ingr in syns1:\n",
    "        if 1 < len(nltk.word_tokenize(ingr)) < 4:\n",
    "            if graph_syn.node[ingr]['represent']:\n",
    "                ngrms = my_ngrams(ingr)\n",
    "                syn_dict = create_syn_dict(ngrms)\n",
    "                for ngrm in ngrms:\n",
    "                    if ngrm in graph_syn and ngrm not in syns1:\n",
    "                        syns2 = representative_syns(ngrm)\n",
    "                        syn_dict[ngrm] = syn_dict[ngrm].union(syns2)\n",
    "                syn_combs = comb_syns(ingr, syn_dict)\n",
    "                for syn_ingr in syn_combs:\n",
    "                    add_node(graph_syn, syn_ingr)\n",
    "                    add_edge(graph_syn, ingr, syn_ingr)\n",
    "        c+=1\n",
    "        if c%100==0:\n",
    "            print(c)\n",
    "\n",
    "# CPU times: user 50min 25s, sys: 3min 47s, total: 54min 12s\n",
    "# Wall time: 54min 10s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data/spanish_representative_ingredients.pickle', 'wb') as f:\n",
    "    pickle.dump(representative_syns_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pures erizos'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'erizo', 'erizos', 'puerco espin', 'puerco espín', 'puercos espines'}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.node_connected_component(graph_syn, 'erizo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pure cerdo espin',\n",
       " 'pure cerdo espines',\n",
       " 'pure cerdo espín',\n",
       " 'pure cerdos espin',\n",
       " 'pure cerdos espines',\n",
       " 'pure cerdos espín',\n",
       " 'pure chancho espines',\n",
       " 'pure chancho espín',\n",
       " 'pure chanchos espin',\n",
       " 'pure chanchos espín',\n",
       " 'pure de cerdo espin',\n",
       " 'pure de cerdo espines',\n",
       " 'pure de cerdo espín',\n",
       " 'pure de cerdos espin',\n",
       " 'pure de cerdos espines',\n",
       " 'pure de cerdos espín',\n",
       " 'pure de chancho espin',\n",
       " 'pure de chancho espines',\n",
       " 'pure de chancho espín',\n",
       " 'pure de chanchos espin',\n",
       " 'pure de chanchos espines',\n",
       " 'pure de chanchos espín',\n",
       " 'pure de erizo',\n",
       " 'pure de erizos',\n",
       " 'pure de puerco espin',\n",
       " 'pure de puerco espines',\n",
       " 'pure de puerco espín',\n",
       " 'pure de puercos espin',\n",
       " 'pure de puercos espines',\n",
       " 'pure de puercos espín',\n",
       " 'pure erizo',\n",
       " 'pure erizos',\n",
       " 'pure puerco espines',\n",
       " 'pure puerco espín',\n",
       " 'pure puercos espines',\n",
       " 'pure puercos espín',\n",
       " 'pures cerdo espin',\n",
       " 'pures cerdo espines',\n",
       " 'pures cerdos espin',\n",
       " 'pures chancho espines',\n",
       " 'pures chancho espín',\n",
       " 'pures chanchos espin',\n",
       " 'pures chanchos espines',\n",
       " 'pures chanchos espín',\n",
       " 'pures de cerdo espin',\n",
       " 'pures de cerdo espines',\n",
       " 'pures de cerdo espín',\n",
       " 'pures de cerdos espin',\n",
       " 'pures de cerdos espines',\n",
       " 'pures de cerdos espín',\n",
       " 'pures de chancho espin',\n",
       " 'pures de chancho espines',\n",
       " 'pures de chancho espín',\n",
       " 'pures de chanchos espin',\n",
       " 'pures de chanchos espines',\n",
       " 'pures de chanchos espín',\n",
       " 'pures de erizo',\n",
       " 'pures de erizos',\n",
       " 'pures de puerco espin',\n",
       " 'pures de puerco espines',\n",
       " 'pures de puerco espín',\n",
       " 'pures de puercos espin',\n",
       " 'pures de puercos espines',\n",
       " 'pures de puercos espín',\n",
       " 'pures erizo',\n",
       " 'pures erizos',\n",
       " 'pures puerco espin',\n",
       " 'pures puerco espines',\n",
       " 'pures puerco espín',\n",
       " 'pures puercos espin',\n",
       " 'puré cerdo espin',\n",
       " 'puré cerdos espin',\n",
       " 'puré cerdos espín',\n",
       " 'puré chancho espines',\n",
       " 'puré chancho espín',\n",
       " 'puré chanchos espines',\n",
       " 'puré chanchos espín',\n",
       " 'puré de cerdo espin',\n",
       " 'puré de cerdo espines',\n",
       " 'puré de cerdo espín',\n",
       " 'puré de cerdos espin',\n",
       " 'puré de cerdos espines',\n",
       " 'puré de cerdos espín',\n",
       " 'puré de chancho espin',\n",
       " 'puré de chancho espines',\n",
       " 'puré de chancho espín',\n",
       " 'puré de chanchos espin',\n",
       " 'puré de chanchos espines',\n",
       " 'puré de chanchos espín',\n",
       " 'puré de erizo',\n",
       " 'puré de erizos',\n",
       " 'puré de puerco espin',\n",
       " 'puré de puerco espines',\n",
       " 'puré de puerco espín',\n",
       " 'puré de puercos espin',\n",
       " 'puré de puercos espines',\n",
       " 'puré de puercos espín',\n",
       " 'puré erizo',\n",
       " 'puré erizos',\n",
       " 'puré puerco espines',\n",
       " 'puré puerco espín',\n",
       " 'puré puercos espin',\n",
       " 'puré puercos espines',\n",
       " 'puré puercos espín',\n",
       " 'purés cerdo espin',\n",
       " 'purés cerdo espines',\n",
       " 'purés cerdos espines',\n",
       " 'purés cerdos espín',\n",
       " 'purés chancho espin',\n",
       " 'purés chancho espín',\n",
       " 'purés chanchos espin',\n",
       " 'purés de cerdo espin',\n",
       " 'purés de cerdo espines',\n",
       " 'purés de cerdo espín',\n",
       " 'purés de cerdos espin',\n",
       " 'purés de cerdos espines',\n",
       " 'purés de cerdos espín',\n",
       " 'purés de chancho espin',\n",
       " 'purés de chancho espines',\n",
       " 'purés de chancho espín',\n",
       " 'purés de chanchos espin',\n",
       " 'purés de chanchos espines',\n",
       " 'purés de chanchos espín',\n",
       " 'purés de erizo',\n",
       " 'purés de erizos',\n",
       " 'purés de puerco espin',\n",
       " 'purés de puerco espines',\n",
       " 'purés de puerco espín',\n",
       " 'purés de puercos espin',\n",
       " 'purés de puercos espines',\n",
       " 'purés de puercos espín',\n",
       " 'purés erizo',\n",
       " 'purés erizos',\n",
       " 'purés puercos espin',\n",
       " 'purés puercos espines',\n",
       " 'purés puercos espín'}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "representative_syns('pures erizo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qqq=list(representative_syns('infusión piña verde pino'))\n",
    "for i in range(len(qqq)-1):\n",
    "    for j in range(i+1,len(qqq)):\n",
    "        if sublist(qqq[i].split(), qqq[j].split()):\n",
    "            print(qqq[i], '<', qqq[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'infusion anana verde pino',\n",
       " 'infusion anana verde pinos',\n",
       " 'infusion anana verdes pino',\n",
       " 'infusion anana verdes pinos',\n",
       " 'infusion ananas verde pino',\n",
       " 'infusion ananas verde pinos',\n",
       " 'infusion ananas verdes pino',\n",
       " 'infusion ananas verdes pinos',\n",
       " 'infusion ananá verde pino',\n",
       " 'infusion ananá verde pinos',\n",
       " 'infusion ananá verdes pino',\n",
       " 'infusion ananá verdes pinos',\n",
       " 'infusion ananás verde pino',\n",
       " 'infusion ananás verde pinos',\n",
       " 'infusion ananás verdes pino',\n",
       " 'infusion ananás verdes pinos',\n",
       " 'infusion de pina verde de pino',\n",
       " 'infusion de piña verde de pino',\n",
       " 'infusion pina verde de pino',\n",
       " 'infusion pina verde pino',\n",
       " 'infusion pina verde pinos',\n",
       " 'infusion pina verdes pino',\n",
       " 'infusion pina verdes pinos',\n",
       " 'infusion pinas verde pino',\n",
       " 'infusion pinas verde pinos',\n",
       " 'infusion pinas verdes de pinos',\n",
       " 'infusion pinas verdes pino',\n",
       " 'infusion pinas verdes pinos',\n",
       " 'infusion piña verde de pino',\n",
       " 'infusion piña verde pino',\n",
       " 'infusion piña verde pinos',\n",
       " 'infusion piña verdes pino',\n",
       " 'infusion piña verdes pinos',\n",
       " 'infusion piñas verde pino',\n",
       " 'infusion piñas verde pinos',\n",
       " 'infusion piñas verdes de pinos',\n",
       " 'infusion piñas verdes pino',\n",
       " 'infusion piñas verdes pinos',\n",
       " 'infusiones anana verde pino',\n",
       " 'infusiones anana verdes pino',\n",
       " 'infusiones anana verdes pinos',\n",
       " 'infusiones ananas verde pino',\n",
       " 'infusiones ananas verde pinos',\n",
       " 'infusiones ananas verdes pino',\n",
       " 'infusiones ananas verdes pinos',\n",
       " 'infusiones ananá verde pino',\n",
       " 'infusiones ananá verde pinos',\n",
       " 'infusiones ananá verdes pino',\n",
       " 'infusiones ananá verdes pinos',\n",
       " 'infusiones ananás verde pino',\n",
       " 'infusiones ananás verde pinos',\n",
       " 'infusiones ananás verdes pino',\n",
       " 'infusiones ananás verdes pinos',\n",
       " 'infusiones de pinas verdes de pinos',\n",
       " 'infusiones de piñas verdes de pinos',\n",
       " 'infusiones pina verde de pino',\n",
       " 'infusiones pina verde pino',\n",
       " 'infusiones pina verdes pino',\n",
       " 'infusiones pina verdes pinos',\n",
       " 'infusiones pinas verde pino',\n",
       " 'infusiones pinas verde pinos',\n",
       " 'infusiones pinas verdes de pinos',\n",
       " 'infusiones pinas verdes pino',\n",
       " 'infusiones pinas verdes pinos',\n",
       " 'infusiones piña verde de pino',\n",
       " 'infusiones piña verde pino',\n",
       " 'infusiones piña verde pinos',\n",
       " 'infusiones piña verdes pino',\n",
       " 'infusiones piña verdes pinos',\n",
       " 'infusiones piñas verde pino',\n",
       " 'infusiones piñas verde pinos',\n",
       " 'infusiones piñas verdes de pinos',\n",
       " 'infusiones piñas verdes pino',\n",
       " 'infusiones piñas verdes pinos',\n",
       " 'infusión anana verde pino',\n",
       " 'infusión anana verde pinos',\n",
       " 'infusión anana verdes pino',\n",
       " 'infusión anana verdes pinos',\n",
       " 'infusión ananas verde pino',\n",
       " 'infusión ananas verde pinos',\n",
       " 'infusión ananas verdes pino',\n",
       " 'infusión ananas verdes pinos',\n",
       " 'infusión ananá verde pino',\n",
       " 'infusión ananá verde pinos',\n",
       " 'infusión ananá verdes pino',\n",
       " 'infusión ananá verdes pinos',\n",
       " 'infusión ananás verde pino',\n",
       " 'infusión ananás verde pinos',\n",
       " 'infusión ananás verdes pino',\n",
       " 'infusión ananás verdes pinos',\n",
       " 'infusión de pina verde de pino',\n",
       " 'infusión de piña verde de pino',\n",
       " 'infusión pina verde de pino',\n",
       " 'infusión pina verde pino',\n",
       " 'infusión pina verde pinos',\n",
       " 'infusión pina verdes pino',\n",
       " 'infusión pina verdes pinos',\n",
       " 'infusión pinas verde pino',\n",
       " 'infusión pinas verde pinos',\n",
       " 'infusión pinas verdes de pinos',\n",
       " 'infusión pinas verdes pino',\n",
       " 'infusión pinas verdes pinos',\n",
       " 'infusión piña verde de pino',\n",
       " 'infusión piña verde pino',\n",
       " 'infusión piña verde pinos',\n",
       " 'infusión piña verdes pino',\n",
       " 'infusión piña verdes pinos',\n",
       " 'infusión piñas verde pino',\n",
       " 'infusión piñas verde pinos',\n",
       " 'infusión piñas verdes de pinos',\n",
       " 'infusión piñas verdes pino',\n",
       " 'infusión piñas verdes pinos'}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "representative_syns('infusión piña verde pino')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-53502a2ba54b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\nc=0\\nfor ingr in list(graph_syn.nodes()):\\n    if 1 < len(nltk.word_tokenize(ingr)) < 5:\\n        syns1 = nx.node_connected_component(graph_syn, ingr)\\n        ngrms = my_ngrams(ingr)\\n        syn_dict = create_syn_dict(ngrms)\\n        for ngrm in ngrms:\\n            if ngrm in graph_syn and ngrm not in syns1:\\n                syns2 = minimal_syns(ngrm)\\n                syn_dict[ngrm] = syn_dict[ngrm].union(syns2)\\n        syn_combs = comb_syns(ingr, syn_dict)\\n        for syn_ingr in syn_combs:\\n            add_node(graph_syn, syn_ingr)\\n            add_edge(graph_syn, ingr, syn_ingr)\\n    c+=1\\n    if c%100==0:\\n        print(c)\\n# CPU times: user 50min 25s, sys: 3min 47s, total: 54min 12s'\\n# Wall time: 54min 10s\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/antonio/virtualenvs/elbulli/lib/python3.4/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2291\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2292\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2293\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2294\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32m/home/antonio/virtualenvs/elbulli/lib/python3.4/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/antonio/virtualenvs/elbulli/lib/python3.4/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1165\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1166\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1167\u001b[1;33m             \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1168\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1169\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-60-3b0752a7b3f7>\u001b[0m in \u001b[0;36mminimal_syns\u001b[1;34m(ingredient)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mok\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msyn2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msyns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0msyn2\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0msyn1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msyn2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msyn1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m                 \u001b[0mok\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/antonio/virtualenvs/elbulli/lib/python3.4/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \"\"\"\n\u001b[1;32m--> 106\u001b[1;33m     return [token for sent in sent_tokenize(text, language)\n\u001b[0m\u001b[0;32m    107\u001b[0m             for token in _treebank_word_tokenize(sent)]\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/antonio/virtualenvs/elbulli/lib/python3.4/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \"\"\"\n\u001b[1;32m---> 90\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/antonio/virtualenvs/elbulli/lib/python3.4/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    767\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0monly\u001b[0m \u001b[0mused\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0mformats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m     \"\"\"\n\u001b[1;32m--> 769\u001b[1;33m     \u001b[0mresource_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize_resource_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    770\u001b[0m     \u001b[0mresource_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    771\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/antonio/virtualenvs/elbulli/lib/python3.4/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mnormalize_resource_url\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'nltk:'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m         \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize_resource_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[1;31m# handled by urllib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/antonio/virtualenvs/elbulli/lib/python3.4/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mnormalize_resource_name\u001b[1;34m(resource_name, allow_relative, relative_path)\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[0mresource_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'^/+'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'/'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresource_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mallow_relative\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[0mresource_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrelative_path\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#OLD\n",
    "\n",
    "# %%time\n",
    "\n",
    "# c=0\n",
    "# for ingr in list(graph_syn.nodes()):\n",
    "#     if 1 < len(nltk.word_tokenize(ingr)) < 5:\n",
    "#         syns1 = nx.node_connected_component(graph_syn, ingr)\n",
    "#         ngrms = my_ngrams(ingr)\n",
    "#         syn_dict = create_syn_dict(ngrms)\n",
    "#         for ngrm in ngrms:\n",
    "#             if ngrm in graph_syn and ngrm not in syns1:\n",
    "#                 syns2 = minimal_syns(ngrm)\n",
    "#                 syn_dict[ngrm] = syn_dict[ngrm].union(syns2)\n",
    "#         syn_combs = comb_syns(ingr, syn_dict)\n",
    "#         for syn_ingr in syn_combs:\n",
    "#             add_node(graph_syn, syn_ingr)\n",
    "#             add_edge(graph_syn, ingr, syn_ingr)\n",
    "#     c+=1\n",
    "#     if c%100==0:\n",
    "#         print(c)\n",
    "# # CPU times: user 50min 25s, sys: 3min 47s, total: 54min 12s'\n",
    "# # Wall time: 54min 10s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21713"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(graph_syn)\n",
    "\n",
    "# 21713"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18635"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_syn.number_of_edges()\n",
    "\n",
    "# 18635"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3078"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nx.number_connected_components(graph_syn)\n",
    "\n",
    "# 2990"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nx.write_gexf(graph_syn, 'data/spanish_ingredients_lexicon_5.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_syn = nx.read_gexf('data/spanish_ingredients_lexicon_5.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
