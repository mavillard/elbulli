{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "import networkx as nx\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ingredients = []\n",
    "with open('data/ingredients/all_ingredients.txt') as f:\n",
    "    for line in f:\n",
    "        ingredient = line.strip()\n",
    "        ingredients.append(ingredient)\n",
    "ingredients = sorted(ingredients, key=len, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['high altitude adjustment for deep fat frying',\n",
       " \"mrs dash's fiesta lime salt-free seasoning\",\n",
       " 'high altitude adjustment for cookie',\n",
       " 'trappist or monastery-style cheese',\n",
       " 'high altitude baking quick bread',\n",
       " \"i can't believe it's not butter\",\n",
       " 'meat: safe cooking temperature',\n",
       " 'african birdseye chile pepper',\n",
       " 'heirloom weight & measurement',\n",
       " 'trompette de la mort mushroom']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingredients[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "techniques = []\n",
    "with open('data/techniques/all_techniques.txt') as f:\n",
    "    for line in f:\n",
    "        technique = line.strip()\n",
    "        techniques.append(technique)\n",
    "techniques = sorted(techniques, key=len, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thermal immersion circulator',\n",
       " 'extreme-heat stir-fry',\n",
       " 'flash pasteurization',\n",
       " 'amylolytic process',\n",
       " 'biomass briquettes',\n",
       " 'coagulated protein',\n",
       " 'high heat stir fry',\n",
       " 'mongolian barbecue',\n",
       " 'culinary triangle',\n",
       " 'food preservation']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "techniques[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def singularize(word):\n",
    "    if word == 'cookies':\n",
    "        result = 'cookie'\n",
    "    elif word == 'mrs':\n",
    "        result = 'mrs'\n",
    "    elif word == 'ras':\n",
    "        result = 'ras'\n",
    "    elif word == 'somen':\n",
    "        result = 'somen'\n",
    "    elif word == 'cos':\n",
    "        result = 'cos'\n",
    "    elif word == 'monks':\n",
    "        result = \"monk's\"\n",
    "    elif word == 'webbs':\n",
    "        result = \"webb's\"\n",
    "    else:\n",
    "        result = WordNetLemmatizer().lemmatize(word.lower())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inf_ing(v_ing):\n",
    "    inf = v_ing\n",
    "    if v_ing == 'de-seeding':\n",
    "        inf = 'de-seed'\n",
    "    elif v_ing == 'de-boning':\n",
    "        inf = 'de-bone'\n",
    "    elif v_ing == 'de-bearding':\n",
    "        inf = 'de-beard'\n",
    "    elif v_ing == 'gutting':\n",
    "        inf = 'gut'\n",
    "    elif v_ing == 'de-glazing':\n",
    "        inf = 'de-glaze'\n",
    "    elif v_ing == 'sautéing':\n",
    "        inf = 'sauté'\n",
    "    elif v_ing == 'degorging':\n",
    "        inf = 'degorge'\n",
    "    elif v_ing == 'stir-frying':\n",
    "        inf = 'stir-fry'\n",
    "    elif v_ing == 'broasting':\n",
    "        inf = 'broast'\n",
    "    elif v_ing == 'juicing':\n",
    "        inf = 'juice'\n",
    "    elif v_ing == 'parbaking':\n",
    "        inf = 'parbake'\n",
    "    elif v_ing == 'charbroiling':\n",
    "        inf = 'charbroil'\n",
    "    elif v_ing == 'crinkle-cutting':\n",
    "        inf = 'crinkle-cut'\n",
    "    elif v_ing == 'cheesemaking':\n",
    "        inf = 'cheesemake'\n",
    "    elif v_ing == 'swissing':\n",
    "        inf = 'swiss'\n",
    "    elif v_ing == 'flash-frying':\n",
    "        inf = 'flash-fry'\n",
    "    elif v_ing == 'flashbaking':\n",
    "        inf = 'flashbake'\n",
    "    else:\n",
    "        inf = WordNetLemmatizer().lemmatize(v_ing.lower(), 'v')\n",
    "    return inf\n",
    "\n",
    "def infinitive(words):\n",
    "    aux = []\n",
    "    word_list = words.split()\n",
    "    for word in word_list:\n",
    "        if word != 'cook' and word != 'cooking':\n",
    "            if word.endswith('ing'):\n",
    "                inf = inf_ing(word.lower())\n",
    "                aux.append(inf)\n",
    "                if inf == word:\n",
    "                    print('NO INFINITIVE FOR', word)\n",
    "            else:\n",
    "                aux.append(word.lower())\n",
    "    result = ' '.join(aux)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ingr_id_graph = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data/ingredients/allrecipes_ingr_ids.csv', 'w') as f1,\\\n",
    "     open('data/techniques/allrecipes_techniques.csv', 'w') as f2:\n",
    "    writer1 = csv.writer(\n",
    "        f1,\n",
    "        delimiter=',',\n",
    "        quotechar='\"',\n",
    "        quoting=csv.QUOTE_MINIMAL\n",
    "    )\n",
    "    writer2 = csv.writer(\n",
    "        f2,\n",
    "        delimiter=',',\n",
    "        quotechar='\"',\n",
    "        quoting=csv.QUOTE_MINIMAL\n",
    "    )\n",
    "    for folder_name in os.listdir('data/allrecipes/'):\n",
    "        folder_path = 'data/allrecipes/' + folder_name\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            file_path = folder_path + '/' + file_name\n",
    "            with open(file_path) as f:\n",
    "                soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "                recipe_info = soup.find('recipe-signup')\n",
    "                recipe_id = recipe_info['data-id']\n",
    "                recipe_title = recipe_info['data-title'].strip('\"').encode('utf_8').decode('unicode_escape')\n",
    "                info_row = [recipe_id, recipe_title]\n",
    "                \n",
    "                ingredient_ids = set()\n",
    "                ingredient_amounts = soup.select('section.recipe-ingredients span[itemprop=ingredients]')\n",
    "                for ingredient_amount in ingredient_amounts:\n",
    "                    ingredient_id = ingredient_amount['data-id']\n",
    "                    ingredient_ids.add(ingredient_id)\n",
    "                    singularized = ' '.join(map(singularize, ingredient_amount.string.strip().split()))\n",
    "                    for ingredient in ingredients:\n",
    "                        if ingredient in singularized:\n",
    "                            if ingr_id_graph.add_edge(ingredient_id, ingredient):\n",
    "                                ingr_id_graph[ingredient_id][ingredient]['weight'] += 1\n",
    "                            else:\n",
    "                                ingr_id_graph.add_edge(ingredient_id, ingredient, weight=1)\n",
    "                ingredient_row = info_row + list(ingredient_ids)\n",
    "                writer1.writerow(ingredient_row)\n",
    "                \n",
    "                technique_names = set()\n",
    "                instructions = soup.select('section.recipe-directions span.recipe-directions__list--item')\n",
    "                for instruction in instructions:\n",
    "                    infinitived = ' '.join(map(infinitive, instruction.string.strip().split()))\n",
    "                    for technique in techniques:\n",
    "                        if technique in infinitived:\n",
    "                            technique_names.add(technique)\n",
    "                technique_row = info_row + list(technique_names)\n",
    "                writer2.writerow(technique_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(recipe_info['data-title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s='Grandma\\u0027s English Muffin Bread'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b=s.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Grandma's English Muffin Bread\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6903\n",
      "\"Grandma\\u0027s English Muffin Bread\"\n",
      "['6903', '\"Grandma\\\\u0027s English Muffin Bread\"', 'bread', 'red', 'grease', 'cover', 'shape', 'ice', 'red', 'brown', 'bake']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('data/allrecipes/6000/6903.html') as f:\n",
    "    soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "    info = soup.find('recipe-signup')\n",
    "    print(info['data-id'])\n",
    "    print(info['data-title'])\n",
    "    qq=info['data-title']\n",
    "    info_row = [info['data-id'], info['data-title']]\n",
    "    technique_names = []\n",
    "    instructions = soup.select('section.recipe-directions span.recipe-directions__list--item')\n",
    "    for instruction in instructions:\n",
    "        infinitived = ' '.join(map(infinitive, instruction.string.strip().split()))\n",
    "        for technique in techniques:\n",
    "            if technique in infinitived:\n",
    "                technique_names.append(technique)\n",
    "    technique_row = info_row + technique_names\n",
    "    print(technique_row)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\"Grandma\\\\u0027s English Muffin Bread\"'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qq.encode('ascii', 'backslashreplace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid normalization form",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-b5934eb7aded>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0municodedata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0municodedata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data-title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NFD'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: invalid normalization form"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "unicodedata.normalize(info['data-title'], 'NFD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Grandma\\\\u0027s English Muffin Bread\"'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(info['data-title']).encode('utf-8').decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('salt', 'VB'), ('the', 'DT'), ('fish', 'NN')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "las tecnicas no tienen pq ser solo verbos - pueden ser sustantivos, por ejemplo, bbq, fondue..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6902\n",
      "\"Indian Naan I\"\n",
      "1 1/2 teaspoons active dry yeast\n",
      "2 cups bread flour\n",
      "1 teaspoon salt\n",
      "9 tablespoons water\n",
      "2 tablespoons clarified butter\n",
      "\n",
      "246279\n",
      "\"White Chocolate Fondue\"\n",
      "1 (12 ounce) package white chocolate chips\n",
      "1/2 cup heavy whipping cream, or more as needed\n",
      "1/4 cup light corn syrup\n",
      "2 tablespoons butter\n",
      "1/4 teaspoon salt\n",
      "1 teaspoon vanilla extract\n",
      "\n",
      "6903\n",
      "\"Grandma\\u0027s English Muffin Bread\"\n",
      "3 cups all-purpose flour\n",
      "2 1/4 teaspoons active dry yeast\n",
      "1/2 tablespoon white sugar\n",
      "1 teaspoon salt\n",
      "1/8 teaspoon baking powder\n",
      "1 cup warm milk\n",
      "1/4 cup water\n",
      "\n",
      "6905\n",
      "\"Apple Scones\"\n",
      "2 cups all-purpose flour\n",
      "1/4 cup white sugar\n",
      "2 teaspoons baking powder\n",
      "1/2 teaspoon baking soda\n",
      "1/2 teaspoon salt\n",
      "1/4 cup butter, chilled\n",
      "1 apple - peeled, cored and shredded\n",
      "1/2 cup milk\n",
      "2 tablespoons milk\n",
      "2 tablespoons white sugar\n",
      "1/2 teaspoon ground cinnamon\n",
      "\n",
      "6904\n",
      "\"Chocolate Filled Muffins\"\n",
      "2 cups all-purpose flour\n",
      "3/4 cup white sugar\n",
      "1/4 cup unsweetened cocoa powder\n",
      "3 teaspoons baking powder\n",
      "1/2 teaspoon salt\n",
      "1/2 teaspoon ground cinnamon\n",
      "1 egg\n",
      "1 cup milk\n",
      "1/3 cup vegetable oil\n",
      "1/4 cup instant powdered milk\n",
      "2 tablespoons hot water\n",
      "1 teaspoon butter\n",
      "1/4 teaspoon almond extract\n",
      "1 cup flaked coconut\n",
      "\n",
      "6901\n",
      "\"Zucchini Pineapple Loaf\"\n",
      "2 eggs\n",
      "1/2 cup vegetable oil\n",
      "1 cup white sugar\n",
      "1 cup grated zucchini\n",
      "1/2 cup crushed pineapple, drained\n",
      "1 teaspoon vanilla extract\n",
      "2 cups all-purpose flour\n",
      "1 teaspoon baking soda\n",
      "1/2 teaspoon baking powder\n",
      "1/2 teaspoon salt\n",
      "3/4 teaspoon ground cinnamon\n",
      "1/4 teaspoon ground nutmeg\n",
      "1/2 cup chopped walnuts\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for folder_name in os.listdir('data/allrecipes/'):\n",
    "    folder_path = 'data/allrecipes/' + folder_name\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = folder_path + '/' + file_name\n",
    "        with open(file_path) as f:\n",
    "            soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "            info = soup.find('recipe-signup')\n",
    "            print(info['data-id'])\n",
    "            print(info['data-title'])\n",
    "            ingredients = soup.select('span[itemprop=ingredients]')\n",
    "            for ingredient in ingredients:\n",
    "                print(ingredient.string.strip())\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
