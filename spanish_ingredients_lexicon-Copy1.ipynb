{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "from itertools import permutations\n",
    "\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import requests\n",
    "from nltk.corpus import cess_esp\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.util import ngrams\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "# client.drop_database('lexicon')\n",
    "db = client.lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with open('data/es_lexicon.csv') as f:\n",
    "#     reader = csv.reader(\n",
    "#         f,\n",
    "#         delimiter=' ',\n",
    "#     )\n",
    "#     docs = []\n",
    "#     count = 0\n",
    "#     for row in reader:\n",
    "#         for i in range(1, len(row[1:]), 2):\n",
    "#             entry = {}\n",
    "#             entry['flexion'] = row[0].lower()\n",
    "#             entry['lemma'] = row[i].lower()\n",
    "#             entry['eagle'] = row[i+1].lower()\n",
    "#             docs.append(entry)\n",
    "#             count += 1\n",
    "#         if count % 1000 == 0:\n",
    "#             db.es_lexicon.insert_many(docs)\n",
    "#             docs = []\n",
    "#     db.es_lexicon.insert_many(docs)\n",
    "#     docs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "668825"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.es_lexicon.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spanish_stopwords = set()\n",
    "with open('data/spanish_stopwords.txt') as f:\n",
    "    for line in f:\n",
    "        word = line.strip()\n",
    "        spanish_stopwords.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tagged_sp_sents = cess_esp.tagged_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# size = int(len(tagged_sp_sents) * 0.1)\n",
    "# train_sp_sents = tagged_sp_sents[size:]\n",
    "# test_sp_sents = tagged_sp_sents[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tagged_sp_words = cess_esp.tagged_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tags = [tag for (word, tag) in tagged_sp_words]\n",
    "# most_freq_tags = nltk.FreqDist(tags)\n",
    "# most_freq_tags.most_common()[:10]\n",
    "\n",
    "# [('sps00', 25272),\n",
    "#  ('ncms000', 11428),\n",
    "#  ('Fc', 11420),\n",
    "#  ('ncfs000', 11008),\n",
    "#  ('da0fs0', 6838),\n",
    "#  ('da0ms0', 6012),\n",
    "#  ('rg', 5937),\n",
    "#  ('Fp', 5866),\n",
    "#  ('cc', 5854),\n",
    "#  ('ncmp000', 5711)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# default_tag = 'ncms000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# t0 = nltk.DefaultTagger(None)\n",
    "# t1 = nltk.UnigramTagger(train_sp_sents, backoff=t0)\n",
    "# t2 = nltk.BigramTagger(train_sp_sents, backoff=t1)\n",
    "# sp_tagger = nltk.TrigramTagger(train_sp_sents, backoff=t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sp_tagger.evaluate(test_sp_sents)\n",
    "\n",
    "# 0.8808231173380034"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('data/sp_tagger.pickle', 'wb') as f:\n",
    "#     pickle.dump(sp_tagger, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data/sp_tagger.pickle', 'rb') as f:\n",
    "    sp_tagger = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_spanish_ingredients_file(filename):\n",
    "    return filename.startswith('es_') and filename.endswith('_ingredients.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_node(g, n):\n",
    "    if not n in g:\n",
    "        g.add_node(n, count=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_edge(g, n1, n2):\n",
    "    if n1 != n2 and not nx.has_path(g, n1, n2):\n",
    "        g.add_edge(n1, n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_syn = nx.Graph()\n",
    "ingredients_root = 'data/ingredients/'\n",
    "for e in os.listdir(ingredients_root):\n",
    "    file_path = ingredients_root + e\n",
    "    if os.path.isfile(file_path):\n",
    "        if is_spanish_ingredients_file(e):\n",
    "            with open(file_path) as f:\n",
    "                for line in f:\n",
    "                    syn_set = set()\n",
    "                    ingrs1 = line.strip()\n",
    "                    for ingrs2 in ingrs1.split(' / '):\n",
    "                        for ingrs3 in ingrs2.split(' o '):\n",
    "                            for ingr in ingrs3.split(' - '):\n",
    "                                syn_set.add(ingr)\n",
    "                                add_node(graph_syn, ingr)\n",
    "                    syn_set = list(syn_set)\n",
    "                    i1 = syn_set[0]\n",
    "                    for i2 in syn_set[1:]:\n",
    "                        add_edge(graph_syn, i1, i2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3327"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_syn.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3222"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.number_connected_components(graph_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nx.write_gexf(graph_syn, 'data/spanish_ingredients_lexicon_1.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_syn = nx.read_gexf('data/spanish_ingredients_lexicon_1.gexf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_mapping = {\n",
    "    'a': 'adj',\n",
    "    'r': 'adv',\n",
    "    'd': 'det',\n",
    "    'n': 'noun',\n",
    "    'v': 'verb',\n",
    "    'p': 'pron',\n",
    "    'c': 'conj',\n",
    "    'i': 'interj',\n",
    "    's': 'prep',\n",
    "    'f': 'punt',\n",
    "    'z': 'num',\n",
    "    'w': 'date-time',\n",
    "}\n",
    "\n",
    "def map_tag(eagle):\n",
    "    return tag_mapping[eagle[0]]\n",
    "\n",
    "def get_category(entry):\n",
    "    if entry['eagle'][0] == 'v' and entry['eagle'][2] == 'p':\n",
    "        categ = 'adj'\n",
    "    else:\n",
    "        categ = map_tag(entry['eagle'])\n",
    "    return categ\n",
    "\n",
    "def has_category(category, entries):\n",
    "    return category in map(get_category, entries)\n",
    "\n",
    "def is_number(x):\n",
    "    return x in ['dos', 'tres', 'cuatro', 'cinco', 'seis', 'siete', 'ocho', 'nueve']\n",
    "\n",
    "def ingredient_tagger(x):\n",
    "    result = []\n",
    "    tokens = nltk.word_tokenize(x)\n",
    "    if len(tokens) == 1:\n",
    "        result.append((x, 'noun'))\n",
    "    else:\n",
    "        tags = sp_tagger.tag(tokens)\n",
    "        for token, tag in tags:\n",
    "            if is_number(token):\n",
    "                tag = 'num'\n",
    "            elif tag:\n",
    "                tag = map_tag(tag.lower())\n",
    "                if tag == 'verb':\n",
    "                    res = list(db.es_lexicon.find({'flexion': token}))\n",
    "                    if res:\n",
    "                        if has_category('adj', res):\n",
    "                            tag = 'adj'\n",
    "                        elif has_category('noun', res):\n",
    "                            tag = 'noun'\n",
    "            else:\n",
    "                res = list(db.es_lexicon.find({'flexion': token}))\n",
    "                if res:\n",
    "                    if has_category('adj', res):\n",
    "                        tag = 'adj'\n",
    "                    elif has_category('noun', res):\n",
    "                        tag = 'noun'\n",
    "                    elif has_category('verb', res):\n",
    "                        tag = 'verb'\n",
    "                    elif has_category('det', res):\n",
    "                        tag = 'det'\n",
    "                    elif has_category('pron', res):\n",
    "                        tag = 'pron'\n",
    "                    elif has_category('prep', res):\n",
    "                        tag = 'prep'\n",
    "                    elif has_category('num', res):\n",
    "                        tag = 'num'\n",
    "                    else:\n",
    "                        tag = get_category(res[0])\n",
    "                else:\n",
    "                    tag = 'noun'\n",
    "            result.append((token, tag))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# with open('data/spanish_ingredients_postags.csv', 'w') as f:\n",
    "#     writer = csv.writer(\n",
    "#         f,\n",
    "#         delimiter=',',\n",
    "#         quotechar='\"',\n",
    "#         quoting=csv.QUOTE_MINIMAL\n",
    "#     )\n",
    "#     for ingredient in graph_syn.nodes_iter():\n",
    "#         pos_tag = ' '.join(tag for token, tag in ingredient_tagger(ingredient))\n",
    "#         row = [ingredient, pos_tag]\n",
    "#         writer.writerow(row)\n",
    "\n",
    "# CPU times: user 6.56 s, sys: 356 ms, total: 6.91 s\n",
    "# Wall time: 22min 14s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "postags = {}\n",
    "with open('data/spanish_ingredients_postags.csv') as f:\n",
    "    reader = csv.reader(\n",
    "        f,\n",
    "        delimiter=',',\n",
    "    )\n",
    "    for row in reader:\n",
    "        postags[row[0]] = row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pimientos', 'noun'), ('verdes', 'adj')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_postags(x):\n",
    "    try:\n",
    "        tags = postags[x]\n",
    "    except:\n",
    "        postags[x] = ' '.join(tag for token, tag in ingredient_tagger(x))\n",
    "        tags = postags[x]\n",
    "    return list(zip(nltk.word_tokenize(x),nltk.word_tokenize(tags)))\n",
    "\n",
    "# Example\n",
    "get_postags('pimientos verdes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# apicultur synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nouns = set()\n",
    "for ingr in graph_syn.nodes_iter():\n",
    "    for token, tag in get_postags(ingr):\n",
    "        if tag == 'noun':\n",
    "            nouns.add(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1620"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with open('data/apicultur_ingredients_synonyms.csv', 'w') as f:\n",
    "#     writer = csv.writer(\n",
    "#         f,\n",
    "#         delimiter=',',\n",
    "#         quotechar='\"',\n",
    "#         quoting=csv.QUOTE_MINIMAL\n",
    "#     )\n",
    "#     base_url = 'https://store.apicultur.com/api/sinonimosporpalabra/1.0.0/'\n",
    "#     headers = {'Authorization': 'Bearer uHS_7Q2Esg7XsUKNsaqFx2sB1mca'}\n",
    "#     count = 0\n",
    "#     for noun in nouns:\n",
    "#         if noun in graph_syn:\n",
    "#             url = base_url + noun\n",
    "#             response = requests.get(url, headers=headers)\n",
    "#             if response.text:\n",
    "#                 js = response.json()\n",
    "#                 row = [noun]\n",
    "#                 for d in js:\n",
    "#                     row.append(d['valor'])\n",
    "#                 writer.writerow(row)\n",
    "#             time.sleep(1)\n",
    "#         count += 1\n",
    "#         if count % 50 == 0:\n",
    "#             time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "apicultur_syns = {}\n",
    "with open('data/apicultur_ingredients_synonyms.csv') as f:\n",
    "    reader = csv.reader(\n",
    "        f,\n",
    "        delimiter=',',\n",
    "    )\n",
    "    for row in reader:\n",
    "        apicultur_syns[row[0]] = row[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def synonyms(x):\n",
    "    return apicultur_syns.get(x, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syns_found = 0\n",
    "for noun in nouns:\n",
    "    syns = synonyms(noun)\n",
    "    for syn in syns:\n",
    "        if syn in graph_syn:\n",
    "            add_edge(graph_syn, noun, syn)\n",
    "            syns_found += 1\n",
    "syns_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3327"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_syn.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3133"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.number_connected_components(graph_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nx.write_gexf(graph_syn, 'data/spanish_ingredients_lexicon_2.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_syn = nx.read_gexf('data/spanish_ingredients_lexicon_2.gexf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Singular and plural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pimientos verdes'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_singular_word(word_tag):\n",
    "    word = word_tag[0]\n",
    "    tag = word_tag[1]\n",
    "    return tag in ['adj', 'noun'] and not word.endswith('s')\n",
    "\n",
    "def is_singular_ingredient(ingredient):\n",
    "    return any(map(is_singular_word, get_postags(ingredient)))\n",
    "\n",
    "def naive_pluralize(word):\n",
    "    if word[-1] in 'aeiou':\n",
    "        plural = word + 's'\n",
    "    elif word[-1] == 'z':\n",
    "        plural = word[:-1] + 'ces'\n",
    "    else:\n",
    "        plural = word + 'es'\n",
    "    return plural\n",
    "\n",
    "def pluralize_adj(word):\n",
    "    if not set('áéíóú').intersection(word):\n",
    "        return naive_pluralize(word)\n",
    "    \n",
    "    singular = word\n",
    "    r1 = db.es_lexicon.find_one({'flexion': word, 'eagle': {'$regex': 'a...s.*'}})\n",
    "    if r1:\n",
    "        lemma = r1['lemma']\n",
    "        eagle = r1['eagle'][:4] + 'p' + r1['eagle'][5:]\n",
    "        r2 = db.es_lexicon.find_one({'lemma': lemma, 'eagle': eagle})\n",
    "        if r2:\n",
    "            singular = r2['flexion']\n",
    "    return singular\n",
    "\n",
    "def pluralize_noun(word):\n",
    "    if not set('áéíóú').intersection(word):\n",
    "        return naive_pluralize(word)\n",
    "    \n",
    "    singular = word\n",
    "    r1 = db.es_lexicon.find_one({'flexion': word, 'eagle': {'$regex': 'n..s.*'}})\n",
    "    if r1:\n",
    "        lemma = r1['lemma']\n",
    "        eagle = r1['eagle'][:3] + 'p' + r1['eagle'][4:]\n",
    "        r2 = db.es_lexicon.find_one({'lemma': lemma, 'eagle': eagle})\n",
    "        if r2:\n",
    "            singular = r2['flexion']\n",
    "    return singular\n",
    "\n",
    "def pluralize_verb(word):\n",
    "    singular = word\n",
    "    r1 = db.es_lexicon.find_one({'flexion': word, 'eagle': {'$regex': 'v....s.*'}})\n",
    "    if r1:\n",
    "        lemma = r1['lemma']\n",
    "        eagle = r1['eagle'][:5] + 'p' + r1['eagle'][6:]\n",
    "        r2 = db.es_lexicon.find_one({'lemma': lemma, 'eagle': eagle})\n",
    "        if r2:\n",
    "            singular = r2['flexion']\n",
    "    return singular\n",
    "\n",
    "def pluralize_det(word):\n",
    "    singular = word\n",
    "    r1 = db.es_lexicon.find_one({'flexion': word, 'eagle': {'$regex': 'd...s.*'}})\n",
    "    if r1:\n",
    "        lemma = r1['lemma']\n",
    "        eagle = r1['eagle'][:4] + 'p' + r1['eagle'][5:]\n",
    "        r2 = db.es_lexicon.find_one({'lemma': lemma, 'eagle': eagle})\n",
    "        if r2:\n",
    "            singular = r2['flexion']\n",
    "    return singular\n",
    "\n",
    "def pluralize_word(word_tag):\n",
    "    word = word_tag[0]\n",
    "    tag = word_tag[1]\n",
    "    plural = word\n",
    "    if word.isalpha():\n",
    "        if tag == 'adj':\n",
    "            plural = pluralize_adj(word)\n",
    "        elif tag == 'noun':\n",
    "            plural = pluralize_noun(word)\n",
    "        elif tag == 'verb':\n",
    "            singular = pluralize_verb(word)\n",
    "        elif tag == 'det':\n",
    "            singular = pluralize_det(word)\n",
    "    return plural\n",
    "\n",
    "def pluralize_ingredient(ingredient):\n",
    "    plurals = map(pluralize_word, get_postags(ingredient))\n",
    "    return ' '.join(plurals)\n",
    "\n",
    "# Example\n",
    "pluralize_ingredient('pimiento verde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pimiento verde'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_plural_word(word_tag):\n",
    "    word = word_tag[0]\n",
    "    tag = word_tag[1]\n",
    "    return tag in ['adj', 'noun'] and word.endswith('s')\n",
    "\n",
    "def is_plural_ingredient(ingredient):\n",
    "    return any(map(is_plural_word, get_postags(ingredient)))\n",
    "\n",
    "def naive_singularize(word):\n",
    "    return word[:-1]\n",
    "\n",
    "def singularize_adj(word):\n",
    "    if not set('áéíóú').intersection(word) and len(word) > 1 and word[-2] in 'aiou':\n",
    "        return naive_singularize(word)\n",
    "    \n",
    "    singular = word\n",
    "    r1 = db.es_lexicon.find_one({'flexion': word, 'eagle': {'$regex': 'a...p.*'}})\n",
    "    if r1:\n",
    "        lemma = r1['lemma']\n",
    "        eagle = r1['eagle'][:4] + 's' + r1['eagle'][5:]\n",
    "        r2 = db.es_lexicon.find_one({'lemma': lemma, 'eagle': eagle})\n",
    "        if r2:\n",
    "            singular = r2['flexion']\n",
    "    return singular\n",
    "\n",
    "def singularize_noun(word):\n",
    "    if not set('áéíóú').intersection(word) and len(word) > 1 and word[-2] in 'aiou':\n",
    "        return naive_singularize(word)\n",
    "    \n",
    "    singular = word\n",
    "    r1 = db.es_lexicon.find_one({'flexion': word, 'eagle': {'$regex': 'n..p.*'}})\n",
    "    if r1:\n",
    "        lemma = r1['lemma']\n",
    "        eagle = r1['eagle'][:3] + 's' + r1['eagle'][4:]\n",
    "        r2 = db.es_lexicon.find_one({'lemma': lemma, 'eagle': eagle})\n",
    "        if r2:\n",
    "            singular = r2['flexion']\n",
    "    return singular\n",
    "\n",
    "def singularize_verb(word):\n",
    "    singular = word\n",
    "    r1 = db.es_lexicon.find_one({'flexion': word, 'eagle': {'$regex': 'v....p.*'}})\n",
    "    if r1:\n",
    "        lemma = r1['lemma']\n",
    "        eagle = r1['eagle'][:5] + 's' + r1['eagle'][6:]\n",
    "        r2 = db.es_lexicon.find_one({'lemma': lemma, 'eagle': eagle})\n",
    "        if r2:\n",
    "            singular = r2['flexion']\n",
    "    return singular\n",
    "\n",
    "def singularize_det(word):\n",
    "    singular = word\n",
    "    r1 = db.es_lexicon.find_one({'flexion': word, 'eagle': {'$regex': 'd...p.*'}})\n",
    "    if r1:\n",
    "        lemma = r1['lemma']\n",
    "        eagle = r1['eagle'][:4] + 's' + r1['eagle'][5:]\n",
    "        r2 = db.es_lexicon.find_one({'lemma': lemma, 'eagle': eagle})\n",
    "        if r2:\n",
    "            singular = r2['flexion']\n",
    "    return singular\n",
    "\n",
    "def singularize_word(word_tag):\n",
    "    word = word_tag[0]\n",
    "    tag = word_tag[1]\n",
    "    singular = word\n",
    "    if word.isalpha():\n",
    "        if tag == 'adj':\n",
    "            singular = singularize_adj(word)\n",
    "        elif tag == 'noun':\n",
    "            singular = singularize_noun(word)\n",
    "        elif tag == 'verb':\n",
    "            singular = singularize_verb(word)\n",
    "        elif tag == 'det':\n",
    "            singular = singularize_det(word)\n",
    "    return singular\n",
    "\n",
    "def singularize_ingredient(ingredient):\n",
    "    singulars = map(singularize_word, get_postags(ingredient))\n",
    "    return ' '.join(singulars)\n",
    "\n",
    "# Example\n",
    "singularize_ingredient('pimientos verdes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# for ingr in graph_syn.nodes():\n",
    "#     if is_singular_ingredient(ingr):\n",
    "#         plural = pluralize_ingredient(ingr)\n",
    "#         add_node(graph_syn, plural)\n",
    "#         add_edge(graph_syn, ingr, plural)\n",
    "#     if is_plural_ingredient(ingr):\n",
    "#         singular = singularize_ingredient(ingr)\n",
    "#         add_node(graph_syn, singular)\n",
    "#         add_edge(graph_syn, ingr, singular)\n",
    "\n",
    "# CPU times: user 8.66 s, sys: 120 ms, total: 8.78 s\n",
    "# Wall time: 8min 5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# len(graph_syn)\n",
    "\n",
    "# 6767"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# graph_syn.number_of_edges()\n",
    "\n",
    "# 3729"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nx.number_connected_components(graph_syn)\n",
    "\n",
    "# 3038"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nx.write_gexf(graph_syn, 'data/spanish_ingredients_lexicon_3.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_syn = nx.read_gexf('data/spanish_ingredients_lexicon_3.gexf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Numbers\n",
    "def numbers(x):\n",
    "    return x.replace(' 1 ', ' uno ') \\\n",
    "            .replace(' 2 ', ' dos ') \\\n",
    "            .replace(' 3 ', ' tres ') \\\n",
    "            .replace(' 4 ', ' cuatro ') \\\n",
    "            .replace(' 5 ', ' cinco ') \\\n",
    "            .replace(' 6 ', ' seis ') \\\n",
    "            .replace(' 7 ', ' siete ') \\\n",
    "            .replace(' 8 ', ' ocho ') \\\n",
    "            .replace(' 9 ', ' nueve ')\n",
    "\n",
    "# Accent marks on vowels - {'á', 'ã', 'ç', 'è', 'é', 'ê', 'í', 'ñ', 'ò', 'ó', 'ú', 'ü', 'ō'}\n",
    "def accent_marks(x):\n",
    "    return x.replace('á', 'a') \\\n",
    "            .replace('ã', 'a') \\\n",
    "            .replace('è', 'e') \\\n",
    "            .replace('é', 'e') \\\n",
    "            .replace('ê', 'e') \\\n",
    "            .replace('í', 'i') \\\n",
    "            .replace('ò', 'o') \\\n",
    "            .replace('ó', 'o') \\\n",
    "            .replace('ō', 'o') \\\n",
    "            .replace('ú', 'u') \\\n",
    "            .replace('ü', 'u')\n",
    "\n",
    "# Non-ascii consonants - {'á', 'ã', 'ç', 'è', 'é', 'ê', 'í', 'ñ', 'ò', 'ó', 'ú', 'ü', 'ō'}\n",
    "def nonascii_consonants(x):\n",
    "    return x.replace('ç', 'c') \\\n",
    "            .replace('ñ', 'n')\n",
    "    \n",
    "# Dashes (-)\n",
    "def dashes1(x):\n",
    "    return x.replace('-', ' ')\n",
    "\n",
    "def dashes2(x):\n",
    "    return x.replace('-', '')\n",
    "\n",
    "# POS tags\n",
    "# ADJETIVOS .... A ADJ ...... X\n",
    "# ADVERBIOS .... R ADV\n",
    "# DETERMINANTES  D DET\n",
    "# NOMBRES ...... N NOUN ..... X\n",
    "# VERBOS ....... V VERB ..... X\n",
    "# PRONOMBRES ... P PRON\n",
    "# CONJUNCIONES . C CONJ\n",
    "# INTERJECCIONES I INTERJ\n",
    "# PREPOSICIONES  S PREP\n",
    "# PUNTUACIÓN ... F PUNTUATION\n",
    "# NUMERALES .... Z NUM ...... X\n",
    "# FECHAS Y HORAS W DATE-TIME\n",
    "def pos_tags(x):\n",
    "    tags = get_postags(x)\n",
    "    filtered = [token\n",
    "                for token, tag in tags\n",
    "                if tag in ['adj', 'noun', 'verb', 'num']\n",
    "               ]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "def itself(x):\n",
    "    return x\n",
    "\n",
    "funcs = [itself, pos_tags, numbers, accent_marks, nonascii_consonants, dashes1, dashes2]\n",
    "combinations = []\n",
    "for i in range(1, len(funcs) + 1):\n",
    "    combinations.append(list(itertools.combinations(funcs, i)))\n",
    "combinations = [c for comb in combinations for c in comb]\n",
    "\n",
    "# def normalize(ingredient): # original time consuming version\n",
    "#     result = set()\n",
    "#     for c in combinations:\n",
    "#         x = ingredient\n",
    "#         for f in c:\n",
    "#             x = f(x)\n",
    "#         result.add(x)\n",
    "#     return result\n",
    "\n",
    "def normalize(ingredient): # dynamic programming version\n",
    "    result = set()\n",
    "    for c in combinations:\n",
    "        x = ingredient\n",
    "        for f in c:\n",
    "            if not x in d[f.__name__]:\n",
    "                d[f.__name__][x] = f(x)\n",
    "            x = d[f.__name__][x]\n",
    "        result.add(x)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([list(map(lambda x: x.__name__, c)) for c in combinations])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# d = defaultdict(dict)\n",
    "\n",
    "# or\n",
    "\n",
    "# with open('data/spanish_ingredients_normalization.pickle', 'rb') as f:\n",
    "#     d = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# for ingr in graph_syn.nodes():\n",
    "#     norms = normalize(ingr)\n",
    "#     for norm in norms:\n",
    "#         add_node(graph_syn, norm)\n",
    "#         add_edge(graph_syn, ingr, norm)\n",
    "\n",
    "# d = dict(d)\n",
    "\n",
    "# CPU times: user 12.4 s, sys: 324 ms, total: 12.8 s\n",
    "# Wall time: 18min 55s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with open('data/spanish_ingredients_normalization.pickle', 'wb') as f:\n",
    "#     pickle.dump(d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# len(graph_syn)\n",
    "\n",
    "# 13124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# graph_syn.number_of_edges()\n",
    "\n",
    "# 10113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nx.number_connected_components(graph_syn)\n",
    "\n",
    "# 3011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nx.write_gexf(graph_syn, 'data/spanish_ingredients_lexicon_4.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_syn = nx.read_gexf('data/spanish_ingredients_lexicon_4.gexf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def my_ngrams(ingredient):\n",
    "    ngrms = []\n",
    "    tokens = nltk.word_tokenize(ingredient)\n",
    "    for i in range(1, len(tokens) + 1):\n",
    "        ngrms.extend(ngrams(tokens, i))\n",
    "    return list(map(lambda x: ' '.join(x), ngrms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lengths = defaultdict(int)\n",
    "for ingr in graph_syn.nodes_iter():\n",
    "    lengths[len(nltk.word_tokenize(ingr))] += 1\n",
    "lengths = dict(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 2041,\n",
       " 2: 4512,\n",
       " 3: 4471,\n",
       " 4: 1210,\n",
       " 5: 578,\n",
       " 6: 205,\n",
       " 7: 51,\n",
       " 8: 25,\n",
       " 9: 23,\n",
       " 10: 8}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'salsa de tomate', 'salsa tomate', 'salsas de tomates', 'salsas tomates'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def minimal_syns(ingredient):\n",
    "    result = set()\n",
    "    syns = nx.node_connected_component(graph_syn, ingredient)\n",
    "    for syn1 in syns:\n",
    "        ok = True\n",
    "        for syn2 in syns:\n",
    "            if syn2 != syn1 and syn2 in nltk.word_tokenize(syn1):\n",
    "                ok = False\n",
    "                break\n",
    "        if ok:\n",
    "            result.add(syn1)\n",
    "    return result\n",
    "\n",
    "# Example\n",
    "minimal_syns('salsa de tomate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['salsa de tomate'],\n",
       " ['salsa', 'de tomate'],\n",
       " ['salsa de', 'tomate'],\n",
       " ['salsa', 'de', 'tomate']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ngram_combinations(ingredient):\n",
    "    combs = []\n",
    "    ngram_list = my_ngrams(ingredient)\n",
    "    for i in range(1, len(ngram_list) + 1):\n",
    "        combs.extend(permutations(ngram_list, i))\n",
    "    combs = [list(c) for c in combs if ' '.join(c) == ingredient]\n",
    "    return combs\n",
    "\n",
    "# Example\n",
    "ngram_combinations('salsa de tomate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['salsas tomatil',\n",
       " 'salsa de tomate',\n",
       " 'salsa de tomates',\n",
       " 'salsa tomatil',\n",
       " 'salsas de tomate',\n",
       " 'salsas de tomates']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def comb_syns(expr, syn_dict):\n",
    "    res = set()\n",
    "    combs = ngram_combinations(expr)\n",
    "    for ngrms in combs:\n",
    "        syn_list = [syn_dict[ngrm] for ngrm in ngrms]\n",
    "        syn_comb = list(product(*syn_list))\n",
    "        for sc in syn_comb:\n",
    "            res.add(' '.join(sc))\n",
    "    return list(res)\n",
    "\n",
    "# Example\n",
    "expr= 'salsa de tomate'\n",
    "syn_dict = {\n",
    "    'salsa': ['salsa', 'salsas'],\n",
    "    'de': ['de'],\n",
    "    'tomate': ['tomate', 'tomates'],\n",
    "    'salsa de': ['salsa de'],\n",
    "    'de tomate': ['de tomate', 'tomatil'],\n",
    "    'salsa de tomate': ['salsa de tomate'],\n",
    "}\n",
    "comb_syns(expr, syn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'de': {'de'},\n",
       " 'de tomate': {'de tomate'},\n",
       " 'salsa': {'salsa'},\n",
       " 'salsa de': {'salsa de'},\n",
       " 'salsa de tomate': {'salsa de tomate'},\n",
       " 'tomate': {'tomate'}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_syn_dict(ngrms):\n",
    "    d = {}\n",
    "    for ngrm in ngrms:\n",
    "        d[ngrm] = set([ngrm])\n",
    "    return d\n",
    "\n",
    "# Example\n",
    "create_syn_dict(my_ngrams('salsa de tomate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-53502a2ba54b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\nc=0\\nfor ingr in list(graph_syn.nodes()):\\n    if 1 < len(nltk.word_tokenize(ingr)) < 5:\\n        syns1 = nx.node_connected_component(graph_syn, ingr)\\n        ngrms = my_ngrams(ingr)\\n        syn_dict = create_syn_dict(ngrms)\\n        for ngrm in ngrms:\\n            if ngrm in graph_syn and ngrm not in syns1:\\n                syns2 = minimal_syns(ngrm)\\n                syn_dict[ngrm] = syn_dict[ngrm].union(syns2)\\n        syn_combs = comb_syns(ingr, syn_dict)\\n        for syn_ingr in syn_combs:\\n            add_node(graph_syn, syn_ingr)\\n            add_edge(graph_syn, ingr, syn_ingr)\\n    c+=1\\n    if c%100==0:\\n        print(c)\\n# CPU times: user 50min 25s, sys: 3min 47s, total: 54min 12s'\\n# Wall time: 54min 10s\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/antonio/virtualenvs/elbulli/lib/python3.4/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2291\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2292\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2293\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2294\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32m/home/antonio/virtualenvs/elbulli/lib/python3.4/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/antonio/virtualenvs/elbulli/lib/python3.4/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1165\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1166\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1167\u001b[1;33m             \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1168\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1169\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-f508323c280c>\u001b[0m in \u001b[0;36mcomb_syns\u001b[1;34m(expr, syn_dict)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcomb_syns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msyn_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mcombs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mngram_combinations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mngrms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcombs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0msyn_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msyn_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mngrm\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mngrm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mngrms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-4a6099f36685>\u001b[0m in \u001b[0;36mngram_combinations\u001b[1;34m(ingredient)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mngram_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_ngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mingredient\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngram_list\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mcombs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpermutations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngram_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mcombs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcombs\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mingredient\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcombs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "c=0\n",
    "for ingr in list(graph_syn.nodes()):\n",
    "    if 1 < len(nltk.word_tokenize(ingr)) < 5:\n",
    "        syns1 = nx.node_connected_component(graph_syn, ingr)\n",
    "        ngrms = my_ngrams(ingr)\n",
    "        syn_dict = create_syn_dict(ngrms)\n",
    "        for ngrm in ngrms:\n",
    "            if ngrm in graph_syn and ngrm not in syns1:\n",
    "                syns2 = minimal_syns(ngrm)\n",
    "                syn_dict[ngrm] = syn_dict[ngrm].union(syns2)\n",
    "        syn_combs = comb_syns(ingr, syn_dict)\n",
    "        for syn_ingr in syn_combs:\n",
    "            add_node(graph_syn, syn_ingr)\n",
    "            add_edge(graph_syn, ingr, syn_ingr)\n",
    "    c+=1\n",
    "    if c%100==0:\n",
    "        print(c)\n",
    "# CPU times: user 50min 25s, sys: 3min 47s, total: 54min 12s'\n",
    "# Wall time: 54min 10s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(graph_syn)\n",
    "\n",
    "# 34548"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_syn.number_of_edges()\n",
    "\n",
    "# 31558"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nx.number_connected_components(graph_syn)\n",
    "\n",
    "# 2990"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nx.write_gexf(graph_syn, 'data/spanish_ingredients_lexicon_5.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_syn = nx.read_gexf('data/spanish_ingredients_lexicon_5.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
